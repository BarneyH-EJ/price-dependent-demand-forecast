{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a6ecaef-27ec-489c-8f28-3115f6f320ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/LGWBHD - Test.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0bf6fa-87a9-4fda-9594-cdf7fa30fce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a25b91ef-33be-4cb8-87c5-009dbf6f43b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01185fb-53fe-4687-8284-9f9ec2965dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "uk_holidays = holidays.UK(years=range(2022, 2026))\n",
    "holidays_df = pd.DataFrame([(date, name) for date, name in uk_holidays.items()], columns=['ds', 'holiday'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3696e98e-4dbb-4521-ab6d-a1e9f7086445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "additional_holidays = pd.DataFrame([\n",
    "    {'ds': '2022-04-18', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2022-08-29', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2023-04-10', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2023-08-28', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2024-04-01', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2024-08-26', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2025-04-21', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2025-08-25', 'holiday': 'Summer Bank Holiday'},\n",
    "])\n",
    "holidays_df = pd.concat([holidays_df, additional_holidays], ignore_index=True)\n",
    "holidays_df.drop_duplicates(inplace=True)\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df.reset_index(drop=True, inplace=True)\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da256da1-01cc-44ef-87e9-91f07b75374e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['charge_dt'] = pd.to_datetime(df['charge_dt'])\n",
    "df['flight_dt'] = pd.to_datetime(df['flight_dt'])\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'charge_dt', 'holiday': 'charge_dt_holiday'}), how='left', on='charge_dt')\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'flight_dt', 'holiday': 'flight_dt_holiday'}), how='left', on='flight_dt')\n",
    "df['is_charge_date_holiday'] = df['charge_dt_holiday'].notnull().astype(int)\n",
    "df['is_flight_date_holiday'] = df['flight_dt_holiday'].notnull().astype(int)\n",
    "df.drop(['charge_dt_holiday', 'flight_dt_holiday'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d18db4-31b9-47c2-b7aa-d3ad9eecd259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['flight_time','sector','route','region','routetype','total_optionality_score','combined_bp','combined_bp_DoW','base','dest','prop_from_base','prop_from_dest','time_quality_score'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bee25fe-d575-4e44-b201-00d9d81604fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['is_charge_date_weekday'] = (df['charge_dt'].dt.dayofweek < 5).astype(int)\n",
    "df['is_flight_date_weekday'] = (df['flight_dt'].dt.dayofweek < 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44ede5c-0bde-4456-b746-4e0f620b44f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Optimisation\n",
    "import numpy as np\n",
    "def optimize_df(df):\n",
    "\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Initial memory usage: {start_mem:.2f} MB\")\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        if 'flightkey' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        elif 'float' in str(df[col].dtype):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "            \n",
    "        elif 'int' in str(df[col].dtype):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(f\"Final memory usage: {end_mem:.2f} MB ({reduction:.2f}% reduction)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "optimize_df(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90fad868-7d1c-4586-ae8b-2d78f732032d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cyclic_cols = ['flight_dow', 'charge_dow', 'flight_dom', 'charge_dom', 'flight_mth', 'charge_mth']\n",
    "\n",
    "def encode_cyclic_features(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        max_val = df[col].max()\n",
    "        df[col + '_sin'] = (np.sin(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df[col + '_cos'] = (np.cos(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = encode_cyclic_features(df, cyclic_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f9d0c8-3bc7-429d-96d4-322ddb963748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['flight_year'] = df['flight_dt'].dt.year.astype(int)\n",
    "df['charge_year'] = df['charge_dt'].dt.year.astype(int)\n",
    "df['day_number'] = (df['charge_dt'] - pd.to_datetime('2022-10-01')).dt.days.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f363aa4c-7c1b-4559-bd00-153c8a7de4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimize_df(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4d9719-a813-417b-89e9-3223af6c5670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['charge_dt','dtg'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d2a4d0-fe38-48d9-8685-b58a8a06b4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f167b9-8b28-4c1b-990d-8a6822247ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['dtg'] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8118c7ad-f707-4b10-bdb3-ae090e341436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.set_index(['flightkey','charge_dt','flight_dt'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad64bec9-6394-43e2-af94-3fbdb0645e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e364caf-dc8e-4633-9a80-e1f3fc2135bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "\n",
    "features_to_scale = ['dtg', 'ty_capacity', 'cumulative_sales','Loadfactor','sales_lag_1','sales_lag_2','sales_lag_3','sales_lag_4','sales_lag_5','sales_lag_6','sales_lag_7','sales_lag_14','sales_lag_21','sales_lag_28','sale_length','sale_period_progress','SF7C7','SF14C14','SF21C21','SF28C28','flight_year','charge_year','day_number']\n",
    "\n",
    "features_already_scaled = ['flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','is_charge_date_holiday','is_flight_date_holiday','is_charge_date_weekday','is_flight_date_weekday']\n",
    "\n",
    "feature_cols = features_to_scale + features_already_scaled\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), features_to_scale)], remainder='passthrough')\n",
    "pipeline = Pipeline([('preprocessor', preprocessor), ('regressor', LinearRegression())])\n",
    "\n",
    "results = []\n",
    "feature_importance_list = []\n",
    "prediction_data = []\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for horizon in range(1, 8):\n",
    "\n",
    "    target_col = f'sales_lead_{horizon}'\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({'Horizon': f'Lead_{horizon}','Fold': fold_idx, 'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "        \n",
    "        model = pipeline.named_steps['regressor']\n",
    "        coefs = model.coef_\n",
    "        transformed_feature_names = features_to_scale + features_already_scaled\n",
    "\n",
    "        batch_df = pd.DataFrame({'Actual': y_test.values, 'Predicted': y_pred}, index=y_test.index).reset_index()\n",
    "        \n",
    "        batch_df['Horizon'] = f'Lead_{horizon}'\n",
    "        batch_df['Fold'] = fold_idx\n",
    "        \n",
    "        prediction_data.append(batch_df)\n",
    "        \n",
    "        for feat_name, coef_val in zip(transformed_feature_names, coefs):\n",
    "            feature_importance_list.append({'Horizon': f'Lead_{horizon}', 'Fold': fold_idx, 'Feature': feat_name, 'Coefficient': coef_val})\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "importance_df = pd.DataFrame(feature_importance_list)\n",
    "all_predictions_df = pd.concat(prediction_data, ignore_index=True)\n",
    "\n",
    "summary_metrics = metrics_df.groupby('Horizon')[['MAE', 'RMSE', 'R2']].mean()\n",
    "\n",
    "print(\"\\n--- Average Performance per Horizon ---\")\n",
    "print(summary_metrics)\n",
    "\n",
    "avg_importance = importance_df.groupby(['Feature', 'Horizon'])['Coefficient'].mean()\n",
    "importance_table = avg_importance.unstack()\n",
    "importance_table = importance_table.abs()\n",
    "importance_table = importance_table.sort_values(by='Lead_1', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance Table ---\")\n",
    "print(importance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbd00b7-d9ac-4910-9540-391f7929cabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "plot_df = all_predictions_df.groupby(['charge_dt', 'Horizon'])[['Actual', 'Predicted']].sum().reset_index()\n",
    "plot_df = plot_df.sort_values('charge_dt')\n",
    "plot_df_long = plot_df.melt(id_vars=['charge_dt', 'Horizon'], value_vars=['Actual', 'Predicted'], var_name='Type', value_name='Sales')\n",
    "\n",
    "fig = px.line(plot_df_long, x='charge_dt', y='Sales', color='Type', facet_col='Horizon', facet_col_wrap=1, title='Total Daily Sales: Actual vs Predicted by Horizon', height=2400, color_discrete_map={'Actual': 'blue', 'Predicted': 'orange'})\n",
    "\n",
    "fig.update_xaxes(matches=None, showticklabels=True) \n",
    "fig.update_yaxes(matches=None) \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a48f78e-08da-4a4a-bbfe-1eed27e2ae96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "features_to_scale = ['dtg', 'ty_capacity', 'cumulative_sales','Loadfactor','sales_lag_1','sales_lag_2','sales_lag_3','sales_lag_4','sales_lag_5','sales_lag_6','sales_lag_7','sales_lag_14','sales_lag_21','sales_lag_28','sale_length','sale_period_progress','SF7C7','SF14C14','SF21C21','SF28C28','flight_year','charge_year','day_number']\n",
    "\n",
    "features_already_scaled = ['flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','is_charge_date_holiday','is_flight_date_holiday','is_charge_date_weekday','is_flight_date_weekday']\n",
    "\n",
    "feature_cols = features_to_scale + features_already_scaled\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), features_to_scale)], remainder='passthrough')\n",
    "pipeline = Pipeline([('preprocessor', preprocessor), ('regressor', Ridge(alpha=1.0))])\n",
    "\n",
    "results = []\n",
    "feature_importance_list = []\n",
    "prediction_data = []\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for horizon in range(1, 8):\n",
    "\n",
    "    target_col = f'sales_lead_{horizon}'\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({'Horizon': f'Lead_{horizon}','Fold': fold_idx, 'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "        \n",
    "        model = pipeline.named_steps['regressor']\n",
    "        coefs = model.coef_\n",
    "        transformed_feature_names = features_to_scale + features_already_scaled\n",
    "\n",
    "        batch_df = pd.DataFrame({'Actual': y_test.values, 'Predicted': y_pred}, index=y_test.index).reset_index()\n",
    "        \n",
    "        batch_df['Horizon'] = f'Lead_{horizon}'\n",
    "        batch_df['Fold'] = fold_idx\n",
    "        \n",
    "        prediction_data.append(batch_df)\n",
    "        \n",
    "        for feat_name, coef_val in zip(transformed_feature_names, coefs):\n",
    "            feature_importance_list.append({'Horizon': f'Lead_{horizon}', 'Fold': fold_idx, 'Feature': feat_name, 'Coefficient': coef_val})\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "importance_df = pd.DataFrame(feature_importance_list)\n",
    "all_predictions_df = pd.concat(prediction_data, ignore_index=True)\n",
    "\n",
    "summary_metrics = metrics_df.groupby('Horizon')[['MAE', 'RMSE', 'R2']].mean()\n",
    "\n",
    "print(\"\\n--- Average Performance per Horizon ---\")\n",
    "print(summary_metrics)\n",
    "\n",
    "avg_importance = importance_df.groupby(['Feature', 'Horizon'])['Coefficient'].mean()\n",
    "importance_table = avg_importance.unstack()\n",
    "importance_table = importance_table.abs()\n",
    "importance_table = importance_table.sort_values(by='Lead_1', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance Table ---\")\n",
    "print(importance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7df4faf-c08c-4582-8da0-f18446c357e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "plot_df = all_predictions_df.groupby(['charge_dt', 'Horizon'])[['Actual', 'Predicted']].sum().reset_index()\n",
    "plot_df = plot_df.sort_values('charge_dt')\n",
    "plot_df_long = plot_df.melt(id_vars=['charge_dt', 'Horizon'], value_vars=['Actual', 'Predicted'], var_name='Type', value_name='Sales')\n",
    "\n",
    "fig = px.line(plot_df_long, x='charge_dt', y='Sales', color='Type', facet_col='Horizon', facet_col_wrap=1, title='Total Daily Sales: Actual vs Predicted by Horizon', height=2400, color_discrete_map={'Actual': 'blue', 'Predicted': 'orange'})\n",
    "\n",
    "fig.update_xaxes(matches=None, showticklabels=True) \n",
    "fig.update_yaxes(matches=None) \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d595e0df-e1d1-455e-88eb-4276533b90ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor \n",
    "\n",
    "features_to_scale = ['dtg', 'ty_capacity', 'cumulative_sales','Loadfactor','sales_lag_1','sales_lag_2','sales_lag_3','sales_lag_4','sales_lag_5','sales_lag_6','sales_lag_7','sales_lag_14','sales_lag_21','sales_lag_28','sale_length','sale_period_progress','SF7C7','SF14C14','SF21C21','SF28C28','flight_year','charge_year','day_number']\n",
    "\n",
    "features_already_scaled = ['flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','is_charge_date_holiday','is_flight_date_holiday','is_charge_date_weekday','is_flight_date_weekday']\n",
    "\n",
    "feature_cols = features_to_scale + features_already_scaled\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', StandardScaler(), features_to_scale)], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([('preprocessor', preprocessor), ('regressor', XGBRegressor(n_estimators=1000, max_depth=5,learning_rate=0.1, verbose=-1, objective='reg:squarederror'))])\n",
    "\n",
    "results = []\n",
    "feature_importance_list = []\n",
    "prediction_data = []\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for horizon in range(1, 8):\n",
    "    target_col = f'sales_lead_{horizon}'\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    for fold_idx, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({'Horizon': f'Lead_{horizon}','Fold': fold_idx, 'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "        \n",
    "        model = pipeline.named_steps['regressor']\n",
    "        importances = model.feature_importances_ \n",
    "        transformed_feature_names = features_to_scale + features_already_scaled\n",
    "\n",
    "        batch_df = pd.DataFrame({'Actual': y_test.values, 'Predicted': y_pred}, index=y_test.index).reset_index()\n",
    "        batch_df['Horizon'] = f'Lead_{horizon}'\n",
    "        batch_df['Fold'] = fold_idx\n",
    "        prediction_data.append(batch_df)\n",
    "        \n",
    "        for feat_name, imp_val in zip(transformed_feature_names, importances):\n",
    "            feature_importance_list.append({'Horizon': f'Lead_{horizon}', 'Fold': fold_idx, 'Feature': feat_name, 'Importance': imp_val})\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "importance_df = pd.DataFrame(feature_importance_list)\n",
    "all_predictions_df = pd.concat(prediction_data, ignore_index=True)\n",
    "\n",
    "summary_metrics = metrics_df.groupby('Horizon')[['MAE', 'RMSE', 'R2']].mean()\n",
    "\n",
    "print(\"\\n--- Average Performance per Horizon (XGBoost) ---\")\n",
    "print(summary_metrics)\n",
    "\n",
    "avg_importance = importance_df.groupby(['Feature', 'Horizon'])['Importance'].mean()\n",
    "importance_table = avg_importance.unstack()\n",
    "importance_table = importance_table.sort_values(by='Lead_1', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance Table ---\")\n",
    "print(importance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c511f66-86d4-4d1d-97a0-4082f53f3bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "plot_df = all_predictions_df.groupby(['charge_dt', 'Horizon'])[['Actual', 'Predicted']].sum().reset_index()\n",
    "plot_df = plot_df.sort_values('charge_dt')\n",
    "plot_df_long = plot_df.melt(id_vars=['charge_dt', 'Horizon'], value_vars=['Actual', 'Predicted'], var_name='Type', value_name='Sales')\n",
    "\n",
    "fig = px.line(plot_df_long, x='charge_dt', y='Sales', color='Type', facet_col='Horizon', facet_col_wrap=1, title='Total Daily Sales: Actual vs Predicted by Horizon', height=2400, color_discrete_map={'Actual': 'blue', 'Predicted': 'orange'})\n",
    "\n",
    "fig.update_xaxes(matches=None, showticklabels=True) \n",
    "fig.update_yaxes(matches=None) \n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apprenticeship Project (final integrated forecast)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
