{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ebaa69-e0f4-4092-a954-8aaea4608ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "from pyspark.sql.functions import to_date, to_timestamp, col, last, when, lit, current_date, date_sub, datediff, substring, sequence, explode, coalesce, sum as spark_sum, min as spark_min\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import holidays\n",
    "import mlflow\n",
    "import re\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "#importing price and meta data\n",
    "\n",
    "price_history = spark.read.table('data_prod.silver_sanezdb.priceinspection').select('segment', 'when', F.col('standardprice').cast('double'))\n",
    "sales_history = spark.read.table('data_experience_commercial.cbt_1423_rtsuite.master_uat').select('flightkey', F.col('charge_dt').cast('date'), 'unt_pre', 'rev_pre', 'chargeproduct', 'dtg', 'bkg_is_ejhsss')\n",
    "dimensions_history = spark.read.table('data_experience_commercial.cbt_0923_segmentfinder.dimensions_history').select('flightkey', 'onsale_dt', 'ty_capacity', 'routetype', 'region', 'flight_dt', 'flight_wk')\n",
    "filtered_dh = dimensions_history.filter((F.col('routetype') == 'Domestic') & (F.col('region') == 'UK-London') & (F.col('flight_dt') >= '2020-01-01') & (F.datediff(F.col('flight_dt'), F.col('onsale_dt')) >= 168))\n",
    "\n",
    "#forward filling flightkey price history to 1d frequency\n",
    "\n",
    "ph_renamed = price_history.withColumnRenamed('when', 'charge_dt').withColumnRenamed('segment', 'flightkey').withColumn('charge_dt', F.col('charge_dt').cast('date')) \n",
    "dph = filtered_dh.join(ph_renamed, on='flightkey', how='inner')\n",
    "dphsmooth = dph.groupby('flightkey','charge_dt').agg(F.avg('standardprice').alias('price'), F.first('flight_dt').alias('flight_dt'), F.first('onsale_dt').alias('onsale_dt')).orderBy('charge_dt')\n",
    "date_range = dphsmooth.groupBy('flightkey').agg(F.min('onsale_dt').alias('start_date'), F.least(F.first('flight_dt'), F.lit(datetime.now().date())).alias('end_date'))\n",
    "index = date_range.withColumn('charge_dt_ts', F.explode(F.sequence(F.col('start_date'), F.col('end_date')))).withColumn('charge_dt', F.col('charge_dt_ts').cast('date')).drop('charge_dt_ts')\n",
    "dphjoin = index.join(dphsmooth, on=['flightkey', 'charge_dt'], how='left').drop('flight_dt', 'onsale_dt')\n",
    "ff_window_spec = Window.partitionBy('flightkey').orderBy('charge_dt')\n",
    "bf_window_spec = Window.partitionBy('flightkey').orderBy('charge_dt').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "dphfilled = dphjoin.withColumn('price', F.coalesce(F.last('price', ignorenulls=True).over(ff_window_spec),F.first('price', ignorenulls=True).over(bf_window_spec))).drop('start_date', 'end_date') #forward fills from first flight price, then back fills from first price if first price date after onsale date\n",
    "finaldph = dphfilled.join(filtered_dh, on='flightkey', how='left').drop('onsale_dt')\n",
    "aggregated_price_history = finaldph.groupby('charge_dt', 'flight_dt').agg(F.avg('price').cast('double').alias('price'))\n",
    "\n",
    "#assembling corresponding sales history\n",
    "\n",
    "dsh = filtered_dh.join(sales_history, on='flightkey', how='left').drop('onsale_dt')\n",
    "filtered_dsh = dsh.filter((F.col('chargeproduct') == 'Ticket') & (F.col('dtg') >= 0) & (F.col('bkg_is_ejhsss') == 0)).drop('chargeproduct', 'dtg', 'bkg_is_ejhsss')\n",
    "dshsmooth = filtered_dsh.groupby('flightkey','charge_dt').agg(F.sum('unt_pre').alias('unt_pre'), F.sum('rev_pre').alias('rev_pre'))\n",
    "dshjoin = index.join(dshsmooth, on=['flightkey', 'charge_dt'], how='left').drop('start_date', 'end_date', 'region', 'routetype', 'flight_dt', 'ty_capacity').fillna(0)\n",
    "window_spec2 = Window.partitionBy('flightkey').orderBy(F.col('charge_dt'))\n",
    "dsh_pax = dshjoin.withColumn('pax_net', F.sum('unt_pre').over(window_spec2))\n",
    "final_dsh = dsh_pax.join(dimensions_history, on='flightkey', how='left').drop('onsale_dt')\n",
    "aggregated_sales_history = final_dsh.groupby('charge_dt', 'flight_dt').agg(F.sum('unt_pre').alias('unt_pre'), F.sum('ty_capacity').alias('ty_capacity'), F.sum('pax_net').alias('pax_net'), F.sum('rev_pre').alias('rev_pre'))\n",
    "\n",
    "#final dataframe\n",
    "\n",
    "df = aggregated_price_history.join(aggregated_sales_history, on=['charge_dt', 'flight_dt'], how='left').toPandas()\n",
    "df['dtg'] = (df['flight_dt'] - df['charge_dt']).dt.days.astype(int)\n",
    "df = df.drop(['pax_net'], axis=1)\n",
    "\n",
    "\n",
    "df.info()\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60134b49-73b9-4f1c-8c92-4e43c35d6e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f0cc4e-ab80-46fe-9203-25bbf91ace00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Optimisation\n",
    "\n",
    "def optimize_df(df):\n",
    "\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Initial memory usage: {start_mem:.2f} MB\")\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        if '_dt' in col:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "        elif 'rev_pre' in col or col.startswith(('R_', 'r_', 'r(target)')):\n",
    "            df[col] = df[col].astype('float32')\n",
    "\n",
    "        elif df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        elif 'float' in str(df[col].dtype):\n",
    "            df[col] = df[col].astype(np.float16)\n",
    "            \n",
    "        elif 'int' in str(df[col].dtype):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(f\"Final memory usage: {end_mem:.2f} MB ({reduction:.2f}% reduction)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "optimize_df(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2273b6c-c73d-4775-b542-cc384815ebe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df[(df['dtg'] <= 275) & (df['dtg'] > 0)]\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "df = df[df['charge_dt'] < current_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130e08a0-96ea-4a54-bc26-4ef5a021296b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Sales Over Charge Date\n",
    "\n",
    "total_sales = df.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "plt.style.use('ggplot')\n",
    "total_sales.plot(style='-', figsize=(20,5), title = 'sales by charge date', y='unt_pre', x='charge_dt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a916c1-99ba-4b28-a259-577cd2c6f4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing covid-19 outlier data before 2022. Keeping Dec 2021 for now to generate lag features.\n",
    "\n",
    "df = df[df['charge_dt'] >= '2021-10-01']\n",
    "total_sales_by_charge_dt = df.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "plt.style.use('ggplot')\n",
    "total_sales_by_charge_dt.plot(style='-', figsize=(20,5), title = 'sales by charge date', y='unt_pre', x='charge_dt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3174e573-2ed1-4864-a358-1b8f89a08042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_by_flight_dt = df.groupby('flight_dt')['unt_pre'].sum().reset_index()\n",
    "sales_by_flight_dt = sales_by_flight_dt[(sales_by_flight_dt['flight_dt'] > '2022-11-01') & (sales_by_flight_dt['flight_dt'] < '2025-11-01')]\n",
    "plt.style.use('ggplot')\n",
    "sales_by_flight_dt.plot(style='-', figsize=(20,5), title = 'sales by flight date', y='unt_pre', x='flight_dt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4663dacf-a4a1-4032-aae8-64d6cd185bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_revenue_by_charge_dt = df.groupby('charge_dt')['rev_pre'].sum().reset_index()\n",
    "plt.style.use('ggplot')\n",
    "total_revenue_by_charge_dt.plot(style='-', figsize=(20,5), title = 'Revenue by charge date', y='rev_pre', x='charge_dt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "325523bc-1d31-4a75-bf9a-1de73ec165a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "revenue_by_flight_dt = df.groupby('flight_dt')['unt_pre'].sum().reset_index()\n",
    "revenue_by_flight_dt = revenue_by_flight_dt[(revenue_by_flight_dt['flight_dt'] > '2022-11-01') & (revenue_by_flight_dt['flight_dt'] < '2025-11-01')]\n",
    "plt.style.use('ggplot')\n",
    "revenue_by_flight_dt.plot(style='-', figsize=(20,5), title = 'sales by flight date', y='unt_pre', x='flight_dt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ee8491-a2c8-4007-a6b3-9ae11e3fa7ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "daily_mean = df.groupby('charge_dt')[['dtg', 'unt_pre', 'price', 'ty_capacity', 'rev_pre']].mean()\n",
    "daily_corr = daily_mean.corr()\n",
    "sns.heatmap(daily_corr, xticklabels=daily_mean.columns,yticklabels=daily_mean.columns, annot=True,fmt=\".2f\",cmap='viridis',linewidths=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc36487-7b0a-477c-927c-2bc694e9b172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "def plot_decomposition(target):\n",
    "    decomposition = seasonal_decompose(target, model='additive')\n",
    "    fig, axes = plt.subplots(4, 1, sharex=True, figsize=(30, 10))\n",
    "    axes[0].plot(target)\n",
    "    axes[1].plot(decomposition.trend)\n",
    "    axes[2].plot(decomposition.seasonal)\n",
    "    axes[3].plot(decomposition.resid)\n",
    "    plt.show()\n",
    "\n",
    "total_sales_by_charge_dt.set_index('charge_dt', inplace=True)\n",
    "plot_decomposition(total_sales_by_charge_dt['unt_pre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d3a2ed-3557-473b-9f08-807db804d9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ACF charge-date\n",
    "df_ACF = df.groupby('charge_dt')['unt_pre'].sum()\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 8))\n",
    "\n",
    "plot_acf(df_ACF, ax=ax1, lags=70)\n",
    "ax1.set_title('Autocorrelation Function (ACF)')\n",
    "\n",
    "plot_pacf(df_ACF, ax=ax2, lags=70, method='ywm') \n",
    "ax2.set_title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04dc715-59c0-41b4-9233-7551663bd86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Typical Booking Curve\n",
    "\n",
    "total_sales_by_dtg = df.groupby('dtg')['unt_pre'].sum().reset_index()\n",
    "plt.style.use('ggplot')\n",
    "total_sales_by_dtg.plot(style='-', figsize=(20,5), title = 'sales by dtg since 2022', y='unt_pre', x='dtg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25deb798-ce28-4204-a9d8-59d04c270f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Booking Curve Over Time\n",
    "\n",
    "df_dynamic_plot = df.reset_index()\n",
    "df_dynamic_plot['charge_dt'] = df_dynamic_plot['charge_dt'].astype(str)\n",
    "df_dynamic_plot = df_dynamic_plot.groupby(['dtg', 'charge_dt'])['unt_pre'].sum().reset_index()\n",
    "fig = px.line(df_dynamic_plot, x='dtg', y='unt_pre', animation_frame='charge_dt', title='Sales by DTG with charge date variations')\n",
    "fig.update_layout(xaxis_title='Days To Go', yaxis_title='Sales', legend_title='Charge Date', height=900, width=1800)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f60afb-447f-4555-9055-42613ab4ae57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def lag_array(df, charge_lags, flight_lags, features_config):\n",
    "\n",
    "    # 1. Create a lookup table with a MultiIndex containing ALL feature columns\n",
    "    feature_names = list(features_config.keys())\n",
    "    df_lookup = df.set_index(['charge_dt', 'flight_dt'])[feature_names]\n",
    "    all_new_features = [df]\n",
    "\n",
    "    # 2. Iterate through each lag combination\n",
    "    for c_lag in charge_lags:\n",
    "        for f_lag in flight_lags:\n",
    "            \n",
    "            target_charge_dts = df['charge_dt'] - pd.to_timedelta(c_lag, unit='d')\n",
    "            target_flight_dts = df['flight_dt'] - pd.to_timedelta(f_lag, unit='d')\n",
    "            target_index = pd.MultiIndex.from_arrays([target_charge_dts, target_flight_dts])\n",
    "\n",
    "            # 3. Perform the lookup. This returns a DataFrame with the lagged values.\n",
    "            lagged_df = df_lookup.reindex(target_index)\n",
    "            lagged_df.index = df.index \n",
    "\n",
    "            # 4. Rename columns with the specified convention \n",
    "            new_column_names = {}\n",
    "            for original_name, prefix in features_config.items():\n",
    "                new_column_names[original_name] = f\"{prefix}_C{c_lag}F{f_lag}\"\n",
    "            \n",
    "            lagged_df = lagged_df.rename(columns=new_column_names)\n",
    "            \n",
    "            all_new_features.append(lagged_df)\n",
    "\n",
    "    # 5. Concatenate all new feature columns\n",
    "    final_df = pd.concat(all_new_features, axis=1)\n",
    "    \n",
    "    # Fill any missing values that resulted from the lookups\n",
    "    final_df.fillna(-1, inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b521b0-832f-4e4f-9628-cfa477be02bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_to_lag = {'unt_pre': 'S', 'rev_pre': 'R'}\n",
    "\n",
    "flight_lags = [-14, -7, 0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70]\n",
    "charge_lags = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70] \n",
    "\n",
    "df = lag_array(df, charge_lags, flight_lags, features_to_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5dbe221-e134-40b3-ab32-ac155050d5cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df, target_col, feature_prefix, title):\n",
    "\n",
    "    # 1. Select the relevant feature columns based on the prefix\n",
    "    feature_cols = [col for col in df.columns if col.startswith(feature_prefix)]\n",
    "    all_cols_for_corr = [target_col] + feature_cols\n",
    "    \n",
    "    # 2. Calculate the correlation matrix for the subset of columns\n",
    "    corr_matrix = df[all_cols_for_corr].corr()\n",
    "    \n",
    "    # Isolate the correlations of the features with the target variable\n",
    "    target_corrs = corr_matrix[target_col].drop(target_col).reset_index()\n",
    "    target_corrs.columns = ['feature', 'correlation']\n",
    "\n",
    "    # 3. Define a function to parse C_lag and F_lag from the new column names\n",
    "    def parse_lags(feature_name):\n",
    "        # Updated regex to handle the prefix, e.g., 'S_C7F0'\n",
    "        match = re.match(rf'{feature_prefix}C(\\d+)F(-?\\d+)', feature_name)\n",
    "        if match:\n",
    "            c_lag = int(match.group(1))\n",
    "            f_lag = int(match.group(2))\n",
    "            return c_lag, f_lag\n",
    "        return None, None\n",
    "\n",
    "    # Apply the parsing function to get the lag values\n",
    "    lags = target_corrs['feature'].apply(parse_lags)\n",
    "    target_corrs[['charge_lag', 'flight_lag']] = pd.DataFrame(lags.tolist(), index=lags.index)\n",
    "\n",
    "    # 4. Pivot the data to create the matrix for the heatmap\n",
    "    corr_pivot = target_corrs.pivot_table(index='charge_lag', columns='flight_lag', values='correlation')\n",
    "\n",
    "    # 5. Plot the heatmap\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    sns.heatmap(corr_pivot,annot=True,fmt=\".2f\",cmap='viridis',linewidths=.1)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Flight Date Lag')\n",
    "    plt.ylabel('Charge Date Lag')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eeda8cb-7ca2-445d-9fad-f028e8ec4d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(df=df, target_col='unt_pre', feature_prefix='S_', title='Correlation of Target (unt_pre) with Sales Lag Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26c51c4-511f-4576-89ed-07f101a9dd2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(df=df, target_col='unt_pre', feature_prefix='R_', title='Correlation of Target (unt_pre) with Revenue Lag Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a47babf-d0a6-47f7-bf85-a8df2381fd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(df=df, target_col='rev_pre', feature_prefix='R_', title='Correlation of Target (rev_pre) with Revenue Lag Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9949dc56-201e-4c4a-895e-dd152211f5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(df=df, target_col='rev_pre', feature_prefix='S_', title='Correlation of Target (rev_pre) with Sales Lag Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f0dc4a-7f6e-4490-a045-6688a9ddf3a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by='dtg')\n",
    "\n",
    "max_price = df['P_C1F0'].max() * 1.05\n",
    "\n",
    "price_bins = pd.cut(df['P_C1F0'], bins=250)\n",
    "grouped_sales = df.groupby(['dtg', price_bins])['unt_pre'].sum()\n",
    "grouped_revenue = df.groupby(['dtg', price_bins])['rev_pre'].sum()\n",
    "\n",
    "max_avg_sales = grouped_sales.max() * 1.05\n",
    "max_avg_revenue = grouped_revenue.max() * 1.05\n",
    "\n",
    "fig_sales = px.histogram(df, x='P_C1F0', y='unt_pre', histfunc='sum', animation_frame='dtg', barmode='group', nbins=250, title='Price vs. Sales (by Days-to-Go)', range_x=[15, 225], range_y=[0,7000])\n",
    "fig_sales.update_layout(xaxis_title=\"Price (£) - Bucketed\", yaxis_title=\"Total Sales (Sum)\")\n",
    "\n",
    "fig_sales.update_layout(height=800, width=1600)\n",
    "fig_sales.show()\n",
    "\n",
    "fig_revenue = px.histogram(df, x='P_C1F0', y='rev_pre', histfunc='sum', animation_frame='dtg', barmode='group', nbins=250, title='Price vs. Revenue (by Days-to-Go)', range_x=[15, 225], range_y=[0, 600000])\n",
    "\n",
    "fig_revenue.update_layout(xaxis_title=\"Price (£) - Bucketed\", yaxis_title=\"Total Revenue (Sum)\")\n",
    "\n",
    "fig_revenue.update_layout(height=800, width=1600)\n",
    "fig_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f8c968-d0cc-4738-b553-f8043e9ba206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_keep = []\n",
    "\n",
    "for col in df.columns:\n",
    "    match = re.search(r'[SR]_C(\\d+)F(-?\\d+)', col)\n",
    "    if match:\n",
    "        c_lag_str = match.group(1)\n",
    "        f_lag_str = match.group(2)\n",
    "        if c_lag_str == f_lag_str:\n",
    "            cols_to_keep.append(col)\n",
    "    else:\n",
    "        cols_to_keep.append(col)\n",
    "\n",
    "df = df[cols_to_keep]\n",
    "\n",
    "# Verify the result\n",
    "print(f\"Original number of columns: {len(df.columns)}\")\n",
    "#print(f\"Number of columns after filtering: {len(df_filtered.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9a7398-5f2a-482d-831b-3cdbadfb5bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['charge_dt'] >= '2022-01-01']\n",
    "df = df[(df['dtg'] <= 200) & (df['dtg'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f23e830-a3b4-4d1d-93f6-f2814f9fa71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seasonality Analysis\n",
    "\n",
    "def create_features(df):\n",
    "    df['flight_month'] = df['flight_dt'].dt.month.astype(int)\n",
    "    df['flight_dow'] = df['flight_dt'].dt.dayofweek.astype(int)\n",
    "    df['flight_dom'] = df['flight_dt'].dt.day.astype(int)\n",
    "    df['flight_doy'] = df['flight_dt'].dt.dayofyear.astype(int)\n",
    "    df['flight_year'] = df['flight_dt'].dt.year.astype(int)\n",
    "    df['charge_month'] = df['charge_dt'].dt.month.astype(int)\n",
    "    df['charge_dow'] = df['charge_dt'].dt.dayofweek.astype(int)\n",
    "    df['charge_dom'] = df['charge_dt'].dt.day.astype(int)\n",
    "    df['charge_doy'] = df['charge_dt'].dt.dayofyear.astype(int)\n",
    "    df['charge_year'] = df['charge_dt'].dt.year.astype(int)\n",
    "    df['day_number'] = (df['charge_dt'] - pd.to_datetime('2022-01-01')).dt.days.astype(int)\n",
    "\n",
    "#create_features(df)\n",
    "#optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a1bbd2-b12e-4540-87b0-98e548329f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flight Month vs Charge Month\n",
    "\n",
    "df_melt = df.melt(id_vars='unt_pre', value_vars=['flight_month', 'charge_month'], var_name='month_type', value_name='month')\n",
    "mean_sales = df_melt.groupby(['month', 'month_type'])['unt_pre'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(data=mean_sales, x='month', y='unt_pre', hue='month_type')\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Historic Sales by Month since 2022')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0c9eb6-c222-4346-a611-dd1ed6cdabf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flight DoW vs Charge DoW\n",
    "\n",
    "df_melt = df.melt(id_vars='unt_pre', value_vars=['flight_dow', 'charge_dow'], var_name='dow_type', value_name='dow')\n",
    "mean_sales = df_melt.groupby(['dow', 'dow_type'])['unt_pre'].mean().reset_index()\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=mean_sales, x='dow', y='unt_pre', hue='dow_type')\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Historic Sales by DoW since 2022')\n",
    "plt.xlabel('DoW')\n",
    "plt.ylabel('Total Sales')\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1b07e5-a271-4c24-9093-d374268dc7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weekly Seasonality\n",
    "\n",
    "df[(df['charge_dt'] > '2024-01-01') & (df['charge_dt'] < '2024-01-31')].groupby('charge_dt')['unt_pre'].sum().plot(figsize=(20,5), title = 'sales by charge date (Jan24)', y='unt_pre', linewidth=10)\n",
    "plt.style.use('ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb96211-9a27-46ee-b8ea-60604111a2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DoM Price vs Sales\n",
    "\n",
    "price_vs_slaes = df.copy()\n",
    "#[(df['charge_month'] != 12) & (df['charge_month'] != 3) & (df['charge_month'] != 4) & ((df['charge_month'] != 1))]\n",
    "\n",
    "mean_sales = price_vs_slaes.groupby('charge_dom')['unt_pre'].mean().reset_index()\n",
    "mean_price = price_vs_slaes.groupby('charge_dom')['price'].mean().reset_index()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "ax1.plot(mean_sales['charge_dom'], mean_sales['unt_pre'], marker='o', linewidth=12, color='tab:red', label='Mean Sales')\n",
    "ax1.set_xlabel('Charge Day of Month')\n",
    "ax1.set_ylabel('Mean Sales', color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(mean_price['charge_dom'], mean_price['price'], marker='s', linewidth=12, color='tab:blue', label='Mean Price')\n",
    "ax2.set_ylabel('Mean Price', color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "fig.suptitle('Mean Sales and Price by Charge Day of Month (2022 - ToDate)')\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(0.9, 0.9))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bc7276-f672-42b5-bb03-0280e6becff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Holiday Features\n",
    "\n",
    "uk_holidays = holidays.UK(years=range(2024, 2026))\n",
    "holidays_df = pd.DataFrame([(date, name) for date, name in uk_holidays.items()], columns=['ds', 'holiday'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5bdae8d-9e7d-446d-bc4d-cb5049e1a953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_curated = holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81336b4-d197-41cd-9e5b-aca123500382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "additional_holidays = pd.DataFrame([\n",
    "    {'ds': '2022-04-18', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2022-08-29', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2023-04-10', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2023-08-28', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2024-04-01', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2024-08-26', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2025-04-21', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2025-08-25', 'holiday': 'Summer Bank Holiday'},\n",
    "])\n",
    "holidays_df = pd.concat([holidays_df, additional_holidays], ignore_index=True)\n",
    "holidays_df.drop_duplicates(inplace=True)\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df.reset_index(drop=True, inplace=True)\n",
    "holidays_df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1be869-8f30-48c2-ab6a-0e2015be9d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_df[\"ds\"] = pd.to_datetime(holidays_df[\"ds\"])\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'charge_dt', 'holiday': 'charge_dt_holiday'}), how='left', on='charge_dt')\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'flight_dt', 'holiday': 'flight_dt_holiday'}), how='left', on='flight_dt')\n",
    "df['is_charge_date_holiday'] = df['charge_dt_holiday'].notnull().astype(int)\n",
    "df['is_flight_date_holiday'] = df['flight_dt_holiday'].notnull().astype(int)\n",
    "df.drop(['charge_dt_holiday', 'flight_dt_holiday'], axis=1, inplace=True)\n",
    "df = df.sort_values(by=['charge_dt', 'dtg'], ascending=[True, True])\n",
    "#df.set_index('charge_dt', inplace=True)\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76fa795-573b-4137-b0cd-631e97a34951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df = original_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020e7cb9-e52b-4e9e-af42-a39731daa604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "original_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234942ec-78b2-462a-8b0a-c62ec4a7fe36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lag_prefixes = ('S_', 'R_')\n",
    "lag_cols = [col for col in df.columns if col.startswith(lag_prefixes)]\n",
    "normal_num_cols = ['dtg', 'ty_capacity'] + lag_cols\n",
    "standard_num_cols=['charge_year','flight_year', 'day_number']\n",
    "cyclic_cols=['flight_dom', 'flight_doy', 'charge_dom', 'charge_doy', 'flight_month', 'charge_month', 'flight_dow', 'charge_dow']\n",
    "index_cols = ['charge_dt', 'flight_dt']\n",
    "def encode_cyclic_features(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        max_val = df[col].max()\n",
    "        df[col + '_sin'] = (np.sin(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df[col + '_cos'] = (np.cos(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = encode_cyclic_features(df, cyclic_cols)\n",
    "\n",
    "for col in index_cols:\n",
    "    df[col] = original_df[col]\n",
    "df.set_index(index_cols, inplace=True)\n",
    "df_preprocessed = df.copy()\n",
    "df = df.drop(['price', 'rev_pre'], axis=1)\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a182da1-6d8f-4f66-b02a-297c77962aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LR train test\n",
    "\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(16, 8), sharex=True)\n",
    "plt.style.use('ggplot')\n",
    "fold = 0\n",
    "preds = []\n",
    "LR_scores = []\n",
    "all_y_test = []\n",
    "all_y_pred_series = []\n",
    "\n",
    "for train_index, val_index in tscv.split(unique_dates):\n",
    "    train_dates = unique_dates[train_index]\n",
    "    val_dates = unique_dates[val_index]\n",
    "\n",
    "    train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "    val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "    train_data = df[train_mask]\n",
    "    val_data = df[val_mask]\n",
    "\n",
    "    total_sales_train = train_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_val = val_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    \n",
    "    ax = axs[fold // 3, fold % 3]\n",
    "\n",
    "    total_sales_train.plot(ax=ax, label='Train', x='charge_dt', y='unt_pre', style='-')\n",
    "    total_sales_val.plot(ax=ax, label='Val', x='charge_dt', y='unt_pre', style='-')\n",
    "    ax.axvline(val_data.index.get_level_values('charge_dt').min(), linestyle='--', color='black')\n",
    "    \n",
    "    end_date = val_dates.max()\n",
    "    start_date = end_date - pd.DateOffset(months=6)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "\n",
    "    ax.set_title(f'Fold {fold+1}')\n",
    "    fold += 1\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "\n",
    "    train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "    train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "\n",
    "    val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "    val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "    X_train = train_data.drop('unt_pre', axis=1)\n",
    "    y_train = train_data['unt_pre']\n",
    "\n",
    "    X_test = val_data.drop('unt_pre', axis=1)\n",
    "    y_test = val_data['unt_pre']\n",
    "\n",
    "    LR_model = LinearRegression()\n",
    "    LR_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = LR_model.predict(X_test)\n",
    "    y_pred_series = pd.Series(y_pred, index=X_test.index, name='unt_pre_pred')\n",
    "    all_y_test.append(y_test)\n",
    "    all_y_pred_series.append(y_pred_series)\n",
    "    preds.extend(y_pred)\n",
    "    LR_score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    LR_scores.append(LR_score)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56287c1-91b6-435c-b558-330b93dc2d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LR Results\n",
    "\n",
    "print(f'individual scores: {LR_scores}')\n",
    "print(f'combined score: {np.mean(LR_scores)}')\n",
    "print(f'std: {np.std(LR_scores)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9eef92b-f0f7-4fb2-84be-6c0ca9e397b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "full_y_test = pd.concat(all_y_test)\n",
    "full_y_pred = pd.concat(all_y_pred_series)\n",
    "y_test_total = full_y_test.groupby(level='charge_dt').sum()\n",
    "y_pred_total = full_y_pred.groupby(level='charge_dt').sum()\n",
    "rmse_LR = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "print(f'Overall RMSE on Daily Totals (All Folds): {rmse_LR:.2f}')\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_total.index, y_test_total, label='Actual Daily Total', marker='.', linestyle='-')\n",
    "plt.plot(y_pred_total.index, y_pred_total, label='Predicted Daily Total', marker='.', linestyle='--')\n",
    "plt.title('LR Model: Actual vs. Predicted Daily Totals (All Folds Combined)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales (unt_pre)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f606bd-1169-4395-b64c-4a08a4f137dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LR Plot (dynamic)\n",
    "\n",
    "results_df = pd.DataFrame({'actual': full_y_test, 'predicted': full_y_pred})\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df['flight_dt'] = pd.to_datetime(results_df['flight_dt'])\n",
    "results_df['charge_dt'] = pd.to_datetime(results_df['charge_dt'])\n",
    "\n",
    "results_df['dtg'] = (results_df['flight_dt'] - results_df['charge_dt']).dt.days\n",
    "results_df['charge_dt'] = results_df['charge_dt'].astype(str)\n",
    "results_df.sort_values(['charge_dt', 'dtg'], inplace=True)\n",
    "\n",
    "agg_df = results_df.groupby(['charge_dt', 'dtg'])[['actual', 'predicted']].sum().reset_index()\n",
    "\n",
    "melted_df = pd.melt(agg_df, id_vars=['charge_dt', 'dtg'], value_vars=['actual', 'predicted'], var_name='sales_type', value_name='sales_value')\n",
    "\n",
    "fig = px.line(melted_df, x='dtg', y='sales_value', color='sales_type', animation_frame='charge_dt', title='LR Model: Actual vs. Predicted Sales by Days To Go (Linear Regression)', color_discrete_map={'actual': 'red', 'predicted': 'blue'})\n",
    "\n",
    "fig.update_layout(height=800, width=1600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1024ffd8-9f31-4377-80a7-be87ff0f7a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LR Revenue model\n",
    "\n",
    "df = df_preprocessed.copy()\n",
    "df = df.drop(['price', 'unt_pre'], axis=1)\n",
    "normal_num_cols = ['dtg', 'ty_capacity']\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb71991-5c9f-4bdf-9f9f-d6fd5b6740f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(16, 8), sharex=True)\n",
    "plt.style.use('ggplot')\n",
    "fold = 0\n",
    "preds = []\n",
    "LR_scores = []\n",
    "all_y_test = []\n",
    "all_y_pred_series = []\n",
    "\n",
    "for train_index, val_index in tscv.split(unique_dates):\n",
    "    train_dates = unique_dates[train_index]\n",
    "    val_dates = unique_dates[val_index]\n",
    "\n",
    "    train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "    val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "    train_data = df[train_mask]\n",
    "    val_data = df[val_mask]\n",
    "\n",
    "    total_sales_train = train_data.groupby('charge_dt')['rev_pre'].sum().reset_index()\n",
    "    total_sales_val = val_data.groupby('charge_dt')['rev_pre'].sum().reset_index()\n",
    "    \n",
    "    ax = axs[fold // 3, fold % 3]\n",
    "\n",
    "    total_sales_train.plot(ax=ax, label='Train', x='charge_dt', y='rev_pre', style='-')\n",
    "    total_sales_val.plot(ax=ax, label='Val', x='charge_dt', y='rev_pre', style='-')\n",
    "    ax.axvline(val_data.index.get_level_values('charge_dt').min(), linestyle='--', color='black')\n",
    "    \n",
    "    end_date = val_dates.max()\n",
    "    start_date = end_date - pd.DateOffset(months=6)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "\n",
    "    ax.set_title(f'Fold {fold+1}')\n",
    "    fold += 1\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "\n",
    "    train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "    train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "\n",
    "    val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "    val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "    X_train = train_data.drop('rev_pre', axis=1)\n",
    "    y_train = train_data['rev_pre']\n",
    "\n",
    "    X_test = val_data.drop('rev_pre', axis=1)\n",
    "    y_test = val_data['rev_pre']\n",
    "\n",
    "    LR_model = LinearRegression()\n",
    "    LR_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = LR_model.predict(X_test)\n",
    "    y_pred_series = pd.Series(y_pred, index=X_test.index, name='unt_pre_pred')\n",
    "    all_y_test.append(y_test)\n",
    "    all_y_pred_series.append(y_pred_series)\n",
    "    preds.extend(y_pred)\n",
    "    LR_score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    LR_scores.append(LR_score)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf084216-44c6-4a88-8410-882a77543196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'individual scores: {LR_scores}')\n",
    "print(f'combined score: {np.mean(LR_scores)}')\n",
    "print(f'std: {np.std(LR_scores)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed9aa447-ce9d-40f9-8242-6b751bddd391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "full_y_test = pd.concat(all_y_test)\n",
    "full_y_pred = pd.concat(all_y_pred_series)\n",
    "y_test_total = full_y_test.groupby(level='charge_dt').sum()\n",
    "y_pred_total = full_y_pred.groupby(level='charge_dt').sum()\n",
    "rmse_LR = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "print(f'Overall RMSE on Daily Totals (All Folds): {rmse_LR:.2f}')\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_total.index, y_test_total, label='Actual Daily Total', marker='.', linestyle='-')\n",
    "plt.plot(y_pred_total.index, y_pred_total, label='Predicted Daily Total', marker='.', linestyle='--')\n",
    "plt.title('LR Model: Actual vs. Predicted Daily Totals (All Folds Combined)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales (unt_pre)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3092eb-0a6e-4fc2-8a55-9b1c19c30549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#custom grid search XGB\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "param_grid = {'max_depth': [7]}\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "tuning_results = {}\n",
    "\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"Testing combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(unique_dates):\n",
    "        train_dates = unique_dates[train_index]\n",
    "        val_dates = unique_dates[val_index]\n",
    "\n",
    "        train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "        val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "        train_data = df[train_mask].copy() \n",
    "        val_data = df[val_mask].copy()\n",
    "\n",
    "        std_scaler = StandardScaler()\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "        train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "        val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "        val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "        X_train = train_data.drop('unt_pre', axis=1)\n",
    "        y_train = train_data['unt_pre']\n",
    "        X_test = val_data.drop('unt_pre', axis=1)\n",
    "        y_test = val_data['unt_pre']\n",
    "\n",
    "        XGB_model = xgb.XGBRegressor(**params, base_score=0.5, booster='gbtree', n_estimators=2000, early_stopping_rounds=50, learning_rate=0.01, objective='reg:pseudohubererror', enable_categorical=True)\n",
    "        XGB_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=100) \n",
    "\n",
    "\n",
    "        y_pred = XGB_model.predict(X_test)\n",
    "        score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        fold_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f\"  -> Mean RMSE across folds: {mean_score:.4f}\\n\")\n",
    "    tuning_results[tuple(params.items())] = mean_score\n",
    "\n",
    "best_params_tuple = min(tuning_results, key=tuning_results.get)\n",
    "best_params = dict(best_params_tuple)\n",
    "best_score = tuning_results[best_params_tuple]\n",
    "\n",
    "print(\"--- Tuning Complete ---\")\n",
    "print(f\"Best Parameters Found: {best_params}\")\n",
    "print(f\"Best Mean RMSE: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61deace3-c354-46be-9f46-8db7cf1b7f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 3, figsize=(16, 8), sharex=True)\n",
    "plt.style.use('ggplot')\n",
    "fold = 0\n",
    "preds = []\n",
    "XGBscores = []\n",
    "all_y_test = []\n",
    "all_y_pred_series = []\n",
    "\n",
    "for train_index, val_index in tscv.split(unique_dates):\n",
    "    train_dates = unique_dates[train_index]\n",
    "    val_dates = unique_dates[val_index]\n",
    "\n",
    "    train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "    val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "    train_data = df[train_mask].copy()\n",
    "    val_data = df[val_mask].copy()\n",
    "    \n",
    "    ax = axs[fold // 3, fold % 3]\n",
    "    total_sales_train = train_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_val = val_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_train.plot(ax=ax, label='Train', x='charge_dt', y='unt_pre', style='-')\n",
    "    total_sales_val.plot(ax=ax, label='Val', x='charge_dt', y='unt_pre', style='-')\n",
    "    ax.axvline(val_data.index.get_level_values('charge_dt').min(), linestyle='--', color='black')\n",
    "    end_date = val_dates.max()\n",
    "    start_date = end_date - pd.DateOffset(months=6)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "    ax.set_title(f'Fold {fold+1}')\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "    train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "    val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "    val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "    X_train = train_data.drop('unt_pre', axis=1)\n",
    "    y_train = train_data['unt_pre']\n",
    "    X_test = val_data.drop('unt_pre', axis=1)\n",
    "    y_test = val_data['unt_pre']\n",
    "\n",
    "    XGB_model_final = xgb.XGBRegressor(**best_params, base_score=0.5, booster='gbtree', n_estimators=2000, early_stopping_rounds=50, learning_rate=0.01, objective='reg:pseudohubererror', enable_categorical=True)\n",
    "    XGB_model_final.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=100)\n",
    "\n",
    "    y_pred = XGB_model_final.predict(X_test)\n",
    "    y_pred_series = pd.Series(y_pred, index=X_test.index, name='unt_pre_pred')\n",
    "    \n",
    "    all_y_test.append(y_test)\n",
    "    all_y_pred_series.append(y_pred_series)\n",
    "    preds.extend(y_pred)\n",
    "    XGBscore = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    XGBscores.append(XGBscore)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nFinal Mean RMSE with best params: {np.mean(XGBscores):.4f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f14fc5-e055-4075-a260-5624280535b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# XGB Results\n",
    "\n",
    "print(f'individual scores: {XGBscores}')\n",
    "print(f'combined score: {np.mean(XGBscores)}')\n",
    "print(f'std: {np.std(XGBscores)}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e438c3-7e1a-4dd2-b8c6-5d0361efba22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#XGB Plot\n",
    "\n",
    "full_y_test = pd.concat(all_y_test)\n",
    "full_y_pred = pd.concat(all_y_pred_series)\n",
    "\n",
    "y_test_total = full_y_test.groupby(level='charge_dt').sum()\n",
    "y_pred_total = full_y_pred.groupby(level='charge_dt').sum()\n",
    "\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "print(f'Overall RMSE on Daily Totals (All Folds): {rmse_xgb:.2f}')\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_total.index, y_test_total, label='Actual Daily Total', marker='.', linestyle='-')\n",
    "plt.plot(y_pred_total.index, y_pred_total, label='Predicted Daily Total', marker='.', linestyle='--')\n",
    "plt.title('XGB Model: Actual vs. Predicted Daily Totals (All Folds Combined)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales (unt_pre)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc2ab91-fadd-4d9c-b8ec-783d6513596a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#XGB Plot (dynamic)\n",
    "\n",
    "results_df = pd.DataFrame({'actual': full_y_test, 'predicted': full_y_pred})\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df['flight_dt'] = pd.to_datetime(results_df['flight_dt'])\n",
    "results_df['charge_dt'] = pd.to_datetime(results_df['charge_dt'])\n",
    "\n",
    "results_df['dtg'] = (results_df['flight_dt'] - results_df['charge_dt']).dt.days\n",
    "results_df['charge_dt'] = results_df['charge_dt'].astype(str)\n",
    "results_df.sort_values(['charge_dt', 'dtg'], inplace=True)\n",
    "\n",
    "agg_df = results_df.groupby(['charge_dt', 'dtg'])[['actual', 'predicted']].sum().reset_index()\n",
    "\n",
    "melted_df = pd.melt(agg_df, id_vars=['charge_dt', 'dtg'], value_vars=['actual', 'predicted'], var_name='sales_type', value_name='sales_value')\n",
    "\n",
    "fig = px.line(melted_df, x='dtg', y='sales_value', color='sales_type', animation_frame='charge_dt', title='XGB Model: Actual vs. Predicted Sales by Days To Go (Linear Regression)', color_discrete_map={'actual': 'red', 'predicted': 'blue'})\n",
    "\n",
    "fig.update_layout(height=800, width=1600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae231d49-060f-451e-b18f-656db133be43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LGBM Sales Forecast\n",
    "\n",
    "#custom grid search lgbm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "param_grid = {'max_depth': [7], 'learning_rate': [0.5]}\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "tuning_results = {}\n",
    "\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"Testing combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(unique_dates):\n",
    "        train_dates = unique_dates[train_index]\n",
    "        val_dates = unique_dates[val_index]\n",
    "\n",
    "        train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "        val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "        train_data = df[train_mask].copy() \n",
    "        val_data = df[val_mask].copy()\n",
    "\n",
    "        std_scaler = StandardScaler()\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "        train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "        val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "        val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "        X_train = train_data.drop('unt_pre', axis=1)\n",
    "        y_train = train_data['unt_pre']\n",
    "        X_test = val_data.drop('unt_pre', axis=1)\n",
    "        y_test = val_data['unt_pre']\n",
    "\n",
    "        LGBM_model = lgb.LGBMRegressor(**params, n_estimators=2000, objective='huber')\n",
    "        LGBM_model.fit(X_train,y_train,eval_set=[(X_test, y_test)],callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=0)])\n",
    "\n",
    "        y_pred = LGBM_model.predict(X_test)\n",
    "        score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        fold_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f\"  -> Mean RMSE across folds: {mean_score:.4f}\\n\")\n",
    "    tuning_results[tuple(params.items())] = mean_score\n",
    "\n",
    "best_params_tuple = min(tuning_results, key=tuning_results.get)\n",
    "best_params = dict(best_params_tuple)\n",
    "best_score = tuning_results[best_params_tuple]\n",
    "\n",
    "print(\"--- Tuning Complete ---\")\n",
    "print(f\"Best Parameters Found: {best_params}\")\n",
    "print(f\"Best Mean RMSE: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71dded92-8efc-40d8-af18-43cca8946bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LGBM CV\n",
    "\n",
    "import lightgbm as lgb\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(16, 8), sharex=True)\n",
    "plt.style.use('ggplot')\n",
    "fold = 0\n",
    "preds = []\n",
    "LGBMscores = []\n",
    "all_y_test = []\n",
    "all_y_pred_series = []\n",
    "\n",
    "for train_index, val_index in tscv.split(unique_dates):\n",
    "    train_dates = unique_dates[train_index]\n",
    "    val_dates = unique_dates[val_index]\n",
    "\n",
    "    train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "    val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "    train_data = df[train_mask].copy()\n",
    "    val_data = df[val_mask].copy()\n",
    "    \n",
    "    ax = axs[fold // 3, fold % 3]\n",
    "    total_sales_train = train_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_val = val_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_train.plot(ax=ax, label='Train', x='charge_dt', y='unt_pre', style='-')\n",
    "    total_sales_val.plot(ax=ax, label='Val', x='charge_dt', y='unt_pre', style='-')\n",
    "    ax.axvline(val_data.index.get_level_values('charge_dt').min(), linestyle='--', color='black')\n",
    "    end_date = val_dates.max()\n",
    "    start_date = end_date - pd.DateOffset(months=6)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "    ax.set_title(f'Fold {fold+1}')\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "    train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "    val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "    val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "    X_train = train_data.drop('unt_pre', axis=1)\n",
    "    y_train = train_data['unt_pre']\n",
    "    X_test = val_data.drop('unt_pre', axis=1)\n",
    "    y_test = val_data['unt_pre']\n",
    "\n",
    "    LGBM_model_final = lgb.LGBMRegressor(max_depth = 7, learning_rate = 0.5, n_estimators=2000, objective='huber')\n",
    "    LGBM_model_final.fit(X_train,y_train,eval_set=[(X_test, y_test)],callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=0)])\n",
    "\n",
    "    y_pred = LGBM_model_final.predict(X_test)\n",
    "    y_pred_series = pd.Series(y_pred, index=X_test.index, name='unt_pre_pred')\n",
    "    \n",
    "    all_y_test.append(y_test)\n",
    "    all_y_pred_series.append(y_pred_series)\n",
    "    preds.extend(y_pred)\n",
    "    LGBMscore = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    LGBMscores.append(LGBMscore)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nFinal Mean RMSE with best params: {np.mean(LGBMscores):.4f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8eff603-829b-4792-aa52-8cae3bede586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# LGBM Results\n",
    "\n",
    "print(f'individual scores: {LGBMscores}')\n",
    "print(f'combined score: {np.mean(LGBMscores)}')\n",
    "print(f'std: {np.std(LGBMscores)}')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21d5e9a-a94c-4eb9-b3c1-2cab546e5b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LGBM Plot\n",
    "\n",
    "full_y_test = pd.concat(all_y_test)\n",
    "full_y_pred = pd.concat(all_y_pred_series)\n",
    "\n",
    "y_test_total = full_y_test.groupby(level='charge_dt').sum()\n",
    "y_pred_total = full_y_pred.groupby(level='charge_dt').sum()\n",
    "\n",
    "rmse_lgbm = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "print(f'Overall RMSE on Daily Totals (All Folds): {rmse_lgbm:.2f}')\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_total.index, y_test_total, label='Actual Daily Total', marker='.', linestyle='-')\n",
    "plt.plot(y_pred_total.index, y_pred_total, label='Predicted Daily Total', marker='.', linestyle='--')\n",
    "plt.title('LGBM Model: Actual vs. Predicted Daily Totals (All Folds Combined)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales (unt_pre)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a551b290-4931-4952-962a-c579f8c7ed65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LGBM Plot (dynamic)\n",
    "\n",
    "results_df = pd.DataFrame({'actual': full_y_test, 'predicted': full_y_pred})\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df['flight_dt'] = pd.to_datetime(results_df['flight_dt'])\n",
    "results_df['charge_dt'] = pd.to_datetime(results_df['charge_dt'])\n",
    "\n",
    "results_df['dtg'] = (results_df['flight_dt'] - results_df['charge_dt']).dt.days\n",
    "results_df['charge_dt'] = results_df['charge_dt'].astype(str)\n",
    "results_df.sort_values(['charge_dt', 'dtg'], inplace=True)\n",
    "\n",
    "agg_df = results_df.groupby(['charge_dt', 'dtg'])[['actual', 'predicted']].sum().reset_index()\n",
    "\n",
    "melted_df = pd.melt(agg_df, id_vars=['charge_dt', 'dtg'], value_vars=['actual', 'predicted'], var_name='sales_type', value_name='sales_value')\n",
    "\n",
    "fig = px.line(melted_df, x='dtg', y='sales_value', color='sales_type', animation_frame='charge_dt', title='LGBM Model: Actual vs. Predicted Sales by Days To Go', color_discrete_map={'actual': 'red', 'predicted': 'blue'})\n",
    "\n",
    "fig.update_layout(height=800, width=1600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee371c74-bfbf-4d3d-b114-2f9a5385e79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LGBM Revenue Forecast\n",
    "\n",
    "df = df_preprocessed.copy()\n",
    "df = df.drop(['price', 'unt_pre'], axis=1)\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "880f10a6-8afe-4060-9ee7-86e5695c4323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LGBM Sales Forecast\n",
    "\n",
    "#custom grid search lgbm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import lightgbm as lgb\n",
    "\n",
    "param_grid = {'max_depth': [7], 'learning_rate': [10, 50, 100]}\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "tuning_results = {}\n",
    "\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"Testing combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(unique_dates):\n",
    "        train_dates = unique_dates[train_index]\n",
    "        val_dates = unique_dates[val_index]\n",
    "\n",
    "        train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "        val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "        train_data = df[train_mask].copy() \n",
    "        val_data = df[val_mask].copy()\n",
    "\n",
    "        std_scaler = StandardScaler()\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "        train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "        val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "        val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "        X_train = train_data.drop('rev_pre', axis=1)\n",
    "        y_train = train_data['rev_pre']\n",
    "        X_test = val_data.drop('rev_pre', axis=1)\n",
    "        y_test = val_data['rev_pre']\n",
    "\n",
    "        LGBM_model = lgb.LGBMRegressor(**params, n_estimators=2000, objective='huber')\n",
    "        LGBM_model.fit(X_train,y_train,eval_set=[(X_test, y_test)],callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=0)])\n",
    "\n",
    "        y_pred = LGBM_model.predict(X_test)\n",
    "        score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        fold_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f\"  -> Mean RMSE across folds: {mean_score:.4f}\\n\")\n",
    "    tuning_results[tuple(params.items())] = mean_score\n",
    "\n",
    "best_params_tuple = min(tuning_results, key=tuning_results.get)\n",
    "best_params = dict(best_params_tuple)\n",
    "best_score = tuning_results[best_params_tuple]\n",
    "\n",
    "print(\"--- Tuning Complete ---\")\n",
    "print(f\"Best Parameters Found: {best_params}\")\n",
    "print(f\"Best Mean RMSE: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8e9c3b-6f84-48ff-a3b8-f40a4707d09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(16, 8), sharex=True)\n",
    "plt.style.use('ggplot')\n",
    "fold = 0\n",
    "preds = []\n",
    "LGBMscores = []\n",
    "all_y_test = []\n",
    "all_y_pred_series = []\n",
    "\n",
    "for train_index, val_index in tscv.split(unique_dates):\n",
    "    train_dates = unique_dates[train_index]\n",
    "    val_dates = unique_dates[val_index]\n",
    "\n",
    "    train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "    val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "    train_data = df[train_mask].copy()\n",
    "    val_data = df[val_mask].copy()\n",
    "    \n",
    "    ax = axs[fold // 3, fold % 3]\n",
    "    total_sales_train = train_data.groupby('charge_dt')['rev_pre'].sum().reset_index()\n",
    "    total_sales_val = val_data.groupby('charge_dt')['rev_pre'].sum().reset_index()\n",
    "    total_sales_train.plot(ax=ax, label='Train', x='charge_dt', y='rev_pre', style='-')\n",
    "    total_sales_val.plot(ax=ax, label='Val', x='charge_dt', y='rev_pre', style='-')\n",
    "    ax.axvline(val_data.index.get_level_values('charge_dt').min(), linestyle='--', color='black')\n",
    "    end_date = val_dates.max()\n",
    "    start_date = end_date - pd.DateOffset(months=6)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "    ax.set_title(f'Fold {fold+1}')\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "    train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "    val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "    val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "    X_train = train_data.drop('rev_pre', axis=1)\n",
    "    y_train = train_data['rev_pre']\n",
    "    X_test = val_data.drop('rev_pre', axis=1)\n",
    "    y_test = val_data['rev_pre']\n",
    "\n",
    "    LGBM_model_final = lgb.LGBMRegressor(max_depth = 7, learning_rate = 50, n_estimators=2000, objective='huber')\n",
    "    LGBM_model_final.fit(X_train,y_train,eval_set=[(X_test, y_test)],callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=0)])\n",
    "\n",
    "    y_pred = LGBM_model_final.predict(X_test)\n",
    "    y_pred_series = pd.Series(y_pred, index=X_test.index, name='rev_pre_pred')\n",
    "    \n",
    "    all_y_test.append(y_test)\n",
    "    all_y_pred_series.append(y_pred_series)\n",
    "    preds.extend(y_pred)\n",
    "    LGBMscore = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    LGBMscores.append(LGBMscore)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nFinal Mean RMSE with best params: {np.mean(LGBMscores):.4f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c196ed02-bc2f-4534-8b68-3fbc4acd97d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f'individual scores: {LGBMscores}')\n",
    "print(f'combined score: {np.mean(LGBMscores)}')\n",
    "print(f'std: {np.std(LGBMscores)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f4ac7e-ac65-4592-a1e4-aedbb2abb3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#LGBM Plot\n",
    "\n",
    "full_y_test = pd.concat(all_y_test)\n",
    "full_y_pred = pd.concat(all_y_pred_series)\n",
    "\n",
    "y_test_total = full_y_test.groupby(level='charge_dt').sum()\n",
    "y_pred_total = full_y_pred.groupby(level='charge_dt').sum()\n",
    "\n",
    "rmse_lgbm = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "print(f'Overall RMSE on Daily Totals (All Folds): {rmse_lgbm:.2f}')\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_total.index, y_test_total, label='Actual Daily Total', marker='.', linestyle='-')\n",
    "plt.plot(y_pred_total.index, y_pred_total, label='Predicted Daily Total', marker='.', linestyle='--')\n",
    "plt.title('LGBM Model: Actual vs. Predicted Daily Totals (All Folds Combined)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales (rev_pre)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80624f38-b28d-43d3-9518-004d2bca4011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RF Grid Search\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {'max_depth': [5]}\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "tuning_results = {}\n",
    "\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"Testing combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(unique_dates):\n",
    "        train_dates = unique_dates[train_index]\n",
    "        val_dates = unique_dates[val_index]\n",
    "\n",
    "        train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "        val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "        train_data = df[train_mask].copy() \n",
    "        val_data = df[val_mask].copy()\n",
    "\n",
    "        std_scaler = StandardScaler()\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "        train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "        val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "        val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "        X_train = train_data.drop('unt_pre', axis=1)\n",
    "        y_train = train_data['unt_pre']\n",
    "        X_test = val_data.drop('unt_pre', axis=1)\n",
    "        y_test = val_data['unt_pre']\n",
    "\n",
    "        RF_model = RandomForestRegressor(**params, n_jobs=-1, n_estimators=50)\n",
    "        RF_model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = RF_model.predict(X_test)\n",
    "        score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        fold_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f\"  -> Mean RMSE across folds: {mean_score:.4f}\\n\")\n",
    "    tuning_results[tuple(params.items())] = mean_score\n",
    "\n",
    "best_params_tuple = min(tuning_results, key=tuning_results.get)\n",
    "best_params = dict(best_params_tuple)\n",
    "best_score = tuning_results[best_params_tuple]\n",
    "\n",
    "print(\"--- Tuning Complete ---\")\n",
    "print(f\"Best Parameters Found: {best_params}\")\n",
    "print(f\"Best Mean RMSE: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350d36b0-a3ee-4773-965b-314e7a865582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RF cv\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(16, 8), sharex=True)\n",
    "plt.style.use('ggplot')\n",
    "fold = 0\n",
    "preds = []\n",
    "RFscores = []\n",
    "all_y_test = []\n",
    "all_y_pred_series = []\n",
    "\n",
    "for train_index, val_index in tscv.split(unique_dates):\n",
    "    train_dates = unique_dates[train_index]\n",
    "    val_dates = unique_dates[val_index]\n",
    "\n",
    "    train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "    val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "    train_data = df[train_mask].copy()\n",
    "    val_data = df[val_mask].copy()\n",
    "    \n",
    "    ax = axs[fold // 3, fold % 3]\n",
    "    total_sales_train = train_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_val = val_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_train.plot(ax=ax, label='Train', x='charge_dt', y='unt_pre', style='-')\n",
    "    total_sales_val.plot(ax=ax, label='Val', x='charge_dt', y='unt_pre', style='-')\n",
    "    ax.axvline(val_data.index.get_level_values('charge_dt').min(), linestyle='--', color='black')\n",
    "    end_date = val_dates.max()\n",
    "    start_date = end_date - pd.DateOffset(months=6)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "    ax.set_title(f'Fold {fold+1}')\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "    train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "    val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "    val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "    X_train = train_data.drop('unt_pre', axis=1)\n",
    "    y_train = train_data['unt_pre']\n",
    "    X_test = val_data.drop('unt_pre', axis=1)\n",
    "    y_test = val_data['unt_pre']\n",
    "\n",
    "    RF_model_final = RandomForestRegressor(**best_params, n_estimators=50, n_jobs=-1)\n",
    "    RF_model_final.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = RF_model_final.predict(X_test)\n",
    "    y_pred_series = pd.Series(y_pred, index=X_test.index, name='unt_pre_pred')\n",
    "    \n",
    "    all_y_test.append(y_test)\n",
    "    all_y_pred_series.append(y_pred_series)\n",
    "    preds.extend(y_pred)\n",
    "    RFscore = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    RFscores.append(RFscore)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nFinal Mean RMSE with best params: {np.mean(RFscores):.4f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c14913-2d22-4775-8303-948e023943bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RF Results\n",
    "\n",
    "print(f'individual scores: {RFscores}')\n",
    "print(f'combined score: {np.mean(RFscores)}')\n",
    "print(f'std: {np.std(RFscores)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d8c4fe-f2a2-4306-a2b0-e3fea9d242be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RF Plot\n",
    "\n",
    "full_y_test = pd.concat(all_y_test)\n",
    "full_y_pred = pd.concat(all_y_pred_series)\n",
    "\n",
    "y_test_total = full_y_test.groupby(level='charge_dt').sum()\n",
    "y_pred_total = full_y_pred.groupby(level='charge_dt').sum()\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "print(f'Overall RMSE on Daily Totals (All Folds): {rmse_rf:.2f}')\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_total.index, y_test_total, label='Actual Daily Total', marker='.', linestyle='-')\n",
    "plt.plot(y_pred_total.index, y_pred_total, label='Predicted Daily Total', marker='.', linestyle='--')\n",
    "plt.title('RF Model: Actual vs. Predicted Daily Totals (All Folds Combined)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales (unt_pre)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e411ddd1-0e96-4cc8-8d90-67ed1f08d530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RF Plot (dynamic)\n",
    "\n",
    "results_df = pd.DataFrame({'actual': full_y_test, 'predicted': full_y_pred})\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df['flight_dt'] = pd.to_datetime(results_df['flight_dt'])\n",
    "results_df['charge_dt'] = pd.to_datetime(results_df['charge_dt'])\n",
    "\n",
    "results_df['dtg'] = (results_df['flight_dt'] - results_df['charge_dt']).dt.days\n",
    "results_df['charge_dt'] = results_df['charge_dt'].astype(str)\n",
    "results_df.sort_values(['charge_dt', 'dtg'], inplace=True)\n",
    "\n",
    "agg_df = results_df.groupby(['charge_dt', 'dtg'])[['actual', 'predicted']].sum().reset_index()\n",
    "\n",
    "melted_df = pd.melt(agg_df, id_vars=['charge_dt', 'dtg'], value_vars=['actual', 'predicted'], var_name='sales_type', value_name='sales_value')\n",
    "\n",
    "fig = px.line(melted_df, x='dtg', y='sales_value', color='sales_type', animation_frame='charge_dt', title='RF Model: Actual vs. Predicted Sales by Days To Go', color_discrete_map={'actual': 'red', 'predicted': 'blue'})\n",
    "\n",
    "fig.update_layout(height=800, width=1600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cbfd0b-b065-41f4-816a-cf9d33b2b8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#AB Grid search CV\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "param_grid = {'n_estimators': [1], 'learning_rate': [0.1, 1.0], 'loss': ['square']}\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "tuning_results = {}\n",
    "\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "tscv = TimeSeriesSplit(n_splits=12, test_size=7)\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"Testing combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(unique_dates):\n",
    "        train_dates = unique_dates[train_index]\n",
    "        val_dates = unique_dates[val_index]\n",
    "        train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "        val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "        train_data = df[train_mask].copy()\n",
    "        val_data = df[val_mask].copy()\n",
    "\n",
    "        X_train = train_data.drop('unt_pre', axis=1)\n",
    "        y_train = train_data['unt_pre']\n",
    "        X_test = val_data.drop('unt_pre', axis=1)\n",
    "        y_test = val_data['unt_pre']\n",
    "\n",
    "        ada_model = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=5),**params)\n",
    "        ada_model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = ada_model.predict(X_test)\n",
    "        score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        fold_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f\"  -> Mean RMSE across folds: {mean_score:.4f}\\n\")\n",
    "    tuning_results[tuple(params.items())] = mean_score\n",
    "\n",
    "best_params_tuple = min(tuning_results, key=tuning_results.get)\n",
    "best_params = dict(best_params_tuple)\n",
    "best_score = tuning_results[best_params_tuple]\n",
    "\n",
    "print(\"--- Tuning Complete ---\")\n",
    "print(f\"Best Parameters Found: {best_params}\")\n",
    "print(f\"Best Mean RMSE: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b1da2b-5475-4306-8d03-bc67b25fb7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#AB cross validation\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(16, 8), sharex=True)\n",
    "plt.style.use('ggplot')\n",
    "fold = 0\n",
    "preds = []\n",
    "ABscores = []\n",
    "all_y_test = []\n",
    "all_y_pred_series = []\n",
    "\n",
    "for train_index, val_index in tscv.split(unique_dates):\n",
    "    train_dates = unique_dates[train_index]\n",
    "    val_dates = unique_dates[val_index]\n",
    "\n",
    "    train_mask = df.index.get_level_values('charge_dt').isin(train_dates)\n",
    "    val_mask = df.index.get_level_values('charge_dt').isin(val_dates)\n",
    "\n",
    "    train_data = df[train_mask].copy()\n",
    "    val_data = df[val_mask].copy()\n",
    "    \n",
    "    ax = axs[fold // 3, fold % 3]\n",
    "    total_sales_train = train_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_val = val_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_train.plot(ax=ax, label='Train', x='charge_dt', y='unt_pre', style='-')\n",
    "    total_sales_val.plot(ax=ax, label='Val', x='charge_dt', y='unt_pre', style='-')\n",
    "    ax.axvline(val_data.index.get_level_values('charge_dt').min(), linestyle='--', color='black')\n",
    "    end_date = val_dates.max()\n",
    "    start_date = end_date - pd.DateOffset(months=6)\n",
    "    ax.set_xlim(start_date, end_date)\n",
    "    ax.set_title(f'Fold {fold+1}')\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    train_data[normal_num_cols] = std_scaler.fit_transform(train_data[normal_num_cols])\n",
    "    train_data[standard_num_cols] = min_max_scaler.fit_transform(train_data[standard_num_cols])\n",
    "    val_data[normal_num_cols] = std_scaler.transform(val_data[normal_num_cols])\n",
    "    val_data[standard_num_cols] = min_max_scaler.transform(val_data[standard_num_cols])\n",
    "\n",
    "    X_train = train_data.drop('unt_pre', axis=1)\n",
    "    y_train = train_data['unt_pre']\n",
    "    X_test = val_data.drop('unt_pre', axis=1)\n",
    "    y_test = val_data['unt_pre']\n",
    "\n",
    "    AB_model_final = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=5), **best_params)\n",
    "    AB_model_final.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = AB_model_final.predict(X_test)\n",
    "    y_pred_series = pd.Series(y_pred, index=X_test.index, name='unt_pre_pred')\n",
    "    \n",
    "    all_y_test.append(y_test)\n",
    "    all_y_pred_series.append(y_pred_series)\n",
    "    preds.extend(y_pred)\n",
    "    ABscore = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    ABscores.append(ABscore)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nFinal Mean RMSE with best params: {np.mean(ABscores):.4f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d3baada-4cb5-4c3f-acd7-750462907284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#AB Results\n",
    "\n",
    "print(f'individual scores: {ABscores}')\n",
    "print(f'combined score: {np.mean(ABscores)}')\n",
    "print(f'std: {np.std(ABscores)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e34439-9355-403c-a2e5-1eafc2da9b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#AB Plot\n",
    "\n",
    "full_y_test = pd.concat(all_y_test)\n",
    "full_y_pred = pd.concat(all_y_pred_series)\n",
    "\n",
    "y_test_total = full_y_test.groupby(level='charge_dt').sum()\n",
    "y_pred_total = full_y_pred.groupby(level='charge_dt').sum()\n",
    "\n",
    "rmse_ab = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "print(f'Overall RMSE on Daily Totals (All Folds): {rmse_ab:.2f}')\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_total.index, y_test_total, label='Actual Daily Total', marker='.', linestyle='-')\n",
    "plt.plot(y_pred_total.index, y_pred_total, label='Predicted Daily Total', marker='.', linestyle='--')\n",
    "plt.title('AB Model: Actual vs. Predicted Daily Totals (All Folds Combined)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales (unt_pre)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432af20e-dac2-4d7e-ad1f-3598e8e0c670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#AB Plot (dynamic)\n",
    "\n",
    "results_df = pd.DataFrame({'actual': full_y_test, 'predicted': full_y_pred})\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df['flight_dt'] = pd.to_datetime(results_df['flight_dt'])\n",
    "results_df['charge_dt'] = pd.to_datetime(results_df['charge_dt'])\n",
    "\n",
    "results_df['dtg'] = (results_df['flight_dt'] - results_df['charge_dt']).dt.days\n",
    "results_df['charge_dt'] = results_df['charge_dt'].astype(str)\n",
    "results_df.sort_values(['charge_dt', 'dtg'], inplace=True)\n",
    "\n",
    "agg_df = results_df.groupby(['charge_dt', 'dtg'])[['actual', 'predicted']].sum().reset_index()\n",
    "\n",
    "melted_df = pd.melt(agg_df, id_vars=['charge_dt', 'dtg'], value_vars=['actual', 'predicted'], var_name='sales_type', value_name='sales_value')\n",
    "\n",
    "fig = px.line(melted_df, x='dtg', y='sales_value', color='sales_type', animation_frame='charge_dt', title='AB Model: Actual vs. Predicted Sales by Days To Go', color_discrete_map={'actual': 'red', 'predicted': 'blue'})\n",
    "\n",
    "fig.update_layout(height=800, width=1600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3ffac4d-bb9a-4b67-9ca5-e026859b931e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Multi Horizon Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96251d89-3091-4f9a-a418-9563a531a1f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_original.copy()\n",
    "df = df.drop('price', axis=1)\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6205cd1-d3f1-4a97-98d9-974d0efbf9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df[(df['dtg'] <= 252) & (df['dtg'] >= 0)]\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "df = df[df['charge_dt'] < current_date]\n",
    "df = df[df['charge_dt'] >= '2021-10-01']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a523a0-489f-42a8-a80e-a56440d82ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def charge_dt_temporal_features(df):\n",
    "    df['charge_dow'] = df['charge_dt'].dt.dayofweek.astype(int)\n",
    "    df['charge_dom'] = df['charge_dt'].dt.day.astype(int)\n",
    "    df['charge_doy'] = df['charge_dt'].dt.dayofyear.astype(int)\n",
    "    df['charge_wom'] = ((df['charge_dt'].dt.day - 1) // 7 + 1).astype(int)\n",
    "    df['charge_woy'] = df['charge_dt'].dt.weekofyear.astype(int)\n",
    "    df['charge_moy'] = df['charge_dt'].dt.month.astype(int)\n",
    "    df['charge_day'] = (df['charge_dt'] - pd.to_datetime('2022-01-01')).dt.days.astype(int)\n",
    "    df['charge_year'] = df['charge_dt'].dt.year.astype(int)\n",
    "\n",
    "def flight_dt_temporal_features(df):\n",
    "    df['flight_dow'] = df['flight_dt'].dt.dayofweek.astype(int)\n",
    "    df['flight_dom'] = df['flight_dt'].dt.day.astype(int)\n",
    "    df['flight_doy'] = df['flight_dt'].dt.dayofyear.astype(int)\n",
    "    df['flight_wom'] = ((df['flight_dt'].dt.day - 1) // 7 + 1).astype(int)\n",
    "    df['flight_woy'] = df['flight_dt'].dt.weekofyear.astype(int)\n",
    "    df['flight_moy'] = df['flight_dt'].dt.month.astype(int)\n",
    "    df['flight_day'] = (df['flight_dt'] - pd.to_datetime('2022-01-01')).dt.days.astype(int)\n",
    "    df['flight_year'] = df['flight_dt'].dt.year.astype(int)\n",
    "\n",
    "charge_dt_temporal_features(df)\n",
    "flight_dt_temporal_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efcd93c-1e66-4118-9f47-8c407e7c13e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uk_holidays = holidays.UK(years=range(2022, 2026))\n",
    "holidays_df = pd.DataFrame([(date, name) for date, name in uk_holidays.items()], columns=['ds', 'holiday'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "additional_holidays = pd.DataFrame([\n",
    "    {'ds': '2022-04-18', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2022-08-29', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2023-04-10', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2023-08-28', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2024-04-01', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2024-08-26', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2025-04-21', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2025-08-25', 'holiday': 'Summer Bank Holiday'},\n",
    "])\n",
    "holidays_df = pd.concat([holidays_df, additional_holidays], ignore_index=True)\n",
    "holidays_df.drop_duplicates(inplace=True)\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df.reset_index(drop=True, inplace=True)\n",
    "holidays_df[\"ds\"] = pd.to_datetime(holidays_df[\"ds\"])\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'charge_dt', 'holiday': 'charge_dt_holiday'}), how='left', on='charge_dt')\n",
    "holidays_df = holidays_df.rename(columns={'ds': 'flight_dt', 'holiday': 'flight_dt_holiday'})\n",
    "holidays_df['flight_dt'] = holidays_df['flight_dt'] - pd.to_timedelta(holidays_df['flight_dt'].dt.dayofweek, unit='d')\n",
    "holidays_df = holidays_df.groupby('flight_dt').agg({'flight_dt_holiday': 'first'}).reset_index()\n",
    "df = df.merge(holidays_df, how='left', on='flight_dt')\n",
    "df['is_charge_date_holiday'] = df['charge_dt_holiday'].notnull().astype(int)\n",
    "df['is_flight_date_holiday'] = df['flight_dt_holiday'].notnull().astype(int)\n",
    "df.drop(['charge_dt_holiday', 'flight_dt_holiday'], axis=1, inplace=True)\n",
    "df = df.sort_values(by=['charge_dt', 'dtg'], ascending=[True, True])\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01064604-3ab3-4688-92a4-67bc06af61cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def lag_array(df, charge_lags, flight_lags, features_config):\n",
    "\n",
    "    # 1. Create a lookup table with a MultiIndex containing ALL feature columns\n",
    "    feature_names = list(features_config.keys())\n",
    "    df_lookup = df.set_index(['charge_dt', 'flight_dt'])[feature_names]\n",
    "    all_new_features = [df]\n",
    "\n",
    "    # 2. Iterate through each lag combination\n",
    "    for c_lag in charge_lags:\n",
    "        for f_lag in flight_lags:\n",
    "            \n",
    "            target_charge_dts = df['charge_dt'] - pd.to_timedelta(c_lag, unit='d')\n",
    "            target_flight_dts = df['flight_dt'] - pd.to_timedelta(f_lag, unit='d')\n",
    "            target_index = pd.MultiIndex.from_arrays([target_charge_dts, target_flight_dts])\n",
    "\n",
    "            # 3. Perform the lookup. This returns a DataFrame with the lagged values.\n",
    "            lagged_df = df_lookup.reindex(target_index)\n",
    "            lagged_df.index = df.index \n",
    "\n",
    "            # 4. Rename columns with the specified convention \n",
    "            new_column_names = {}\n",
    "            for original_name, prefix in features_config.items():\n",
    "                new_column_names[original_name] = f\"{prefix}_C{c_lag}F{f_lag}\"\n",
    "            \n",
    "            lagged_df = lagged_df.rename(columns=new_column_names)\n",
    "            \n",
    "            all_new_features.append(lagged_df)\n",
    "\n",
    "    # 5. Concatenate all new feature columns\n",
    "    final_df = pd.concat(all_new_features, axis=1)\n",
    "    \n",
    "    # Fill any missing values that resulted from the lookups\n",
    "    final_df.fillna(-1, inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14f91a4-3236-463b-8268-9bfac701f404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Long-Term Lags\n",
    "\n",
    "features_to_lag = {'unt_pre': 'S', 'rev_pre': 'R'}\n",
    "\n",
    "flight_lags = [-14, -7, 0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70]\n",
    "charge_lags = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70] \n",
    "\n",
    "df = lag_array(df, charge_lags, flight_lags, features_to_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4299d5-40e1-4575-ba07-fe81dbc2f8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df, target_col, feature_prefix, title):\n",
    "\n",
    "    # 1. Select the relevant feature columns based on the prefix\n",
    "    feature_cols = [col for col in df.columns if col.startswith(feature_prefix)]\n",
    "    all_cols_for_corr = [target_col] + feature_cols\n",
    "    \n",
    "    # 2. Calculate the correlation matrix for the subset of columns\n",
    "    corr_matrix = df[all_cols_for_corr].corr()\n",
    "    \n",
    "    # Isolate the correlations of the features with the target variable\n",
    "    target_corrs = corr_matrix[target_col].drop(target_col).reset_index()\n",
    "    target_corrs.columns = ['feature', 'correlation']\n",
    "\n",
    "    # 3. Define a function to parse C_lag and F_lag from the new column names\n",
    "    def parse_lags(feature_name):\n",
    "        # Updated regex to handle the prefix, e.g., 'S_C7F0'\n",
    "        match = re.match(rf'{feature_prefix}C(\\d+)F(-?\\d+)', feature_name)\n",
    "        if match:\n",
    "            c_lag = int(match.group(1))\n",
    "            f_lag = int(match.group(2))\n",
    "            return c_lag, f_lag\n",
    "        return None, None\n",
    "\n",
    "    # Apply the parsing function to get the lag values\n",
    "    lags = target_corrs['feature'].apply(parse_lags)\n",
    "    target_corrs[['charge_lag', 'flight_lag']] = pd.DataFrame(lags.tolist(), index=lags.index)\n",
    "\n",
    "    # 4. Pivot the data to create the matrix for the heatmap\n",
    "    corr_pivot = target_corrs.pivot_table(index='charge_lag', columns='flight_lag', values='correlation')\n",
    "\n",
    "    # 5. Plot the heatmap\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    sns.heatmap(corr_pivot,annot=True,fmt=\".2f\",cmap='viridis',linewidths=.1)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Flight Date Lag')\n",
    "    plt.ylabel('Charge Date Lag')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c10197-62b3-4204-b33c-ed3782208c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(df=df, target_col='unt_pre', feature_prefix='S_', title='Correlation of Target (unt_pre) with Sales Lag Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e444b31d-fd17-4453-8f4a-2bd9ba423105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_keep = []\n",
    "\n",
    "for col in df.columns:\n",
    "    match = re.search(r'[SR]_C(\\d+)F(-?\\d+)', col)\n",
    "    if match:\n",
    "        c_lag_str = match.group(1)\n",
    "        f_lag_str = match.group(2)\n",
    "        if c_lag_str == f_lag_str:\n",
    "            cols_to_keep.append(col)\n",
    "    else:\n",
    "        cols_to_keep.append(col)\n",
    "\n",
    "df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81a75a5-0f5d-45f0-a4c3-348888dd5f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Short-Term Lags\n",
    "\n",
    "features_to_lag = {'unt_pre': 's', 'rev_pre': 'r'}\n",
    "\n",
    "flight_lags = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "charge_lags = [0, 1, 2, 3, 4, 5, 6, 7, 8] \n",
    "\n",
    "df = lag_array(df, charge_lags, flight_lags, features_to_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8f1f49-1a52-4256-89d8-5224b27f2d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_correlation_heatmap(df=df, target_col='unt_pre', feature_prefix='s_', title='Correlation of Target (unt_pre) with Sales Lag Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ddf3157-fbd1-4f73-b89a-c3f841ab86e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_keep = []\n",
    "\n",
    "for col in df.columns:\n",
    "    match = re.search(r'[sr]_C(\\d+)F(-?\\d+)', col)\n",
    "    if match:\n",
    "        c_lag_str = match.group(1)\n",
    "        f_lag_str = match.group(2)\n",
    "        if (int(f_lag_str) == 0) & (int(c_lag_str) < 8) & (int(c_lag_str) > 0):\n",
    "            cols_to_keep.append(col)\n",
    "    else:\n",
    "        cols_to_keep.append(col)\n",
    "\n",
    "df = df[cols_to_keep]\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c78bdfa-8c1b-4923-9db7-41184f2f7765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['charge_dt'] >= '2022-01-01']\n",
    "df = df[(df['dtg'] <= 168) & (df['dtg'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef07266-2b7f-46e9-98e8-f6ac995c123a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "original_df = df.copy()\n",
    "\n",
    "lag_prefixes = ('S_', 'R_', 's_', 'r_')\n",
    "lag_cols = [col for col in df.columns if col.startswith(lag_prefixes)]\n",
    "\n",
    "index_cols = ['charge_dt', 'flight_dt']\n",
    "normal_num_cols = ['dtg', 'ty_capacity'] + lag_cols\n",
    "standard_num_cols=['charge_year','flight_year', 'flight_day', 'charge_day']\n",
    "cyclic_cols=['flight_dow', 'flight_dom', 'flight_doy', 'flight_wom', 'flight_woy', 'flight_moy', \n",
    "             'charge_dow', 'charge_dom', 'charge_doy', 'charge_wom', 'charge_woy', 'charge_moy']\n",
    "\n",
    "def encode_cyclic_features(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        max_val = df[col].max()\n",
    "        df[col + '_sin'] = (np.sin(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df[col + '_cos'] = (np.cos(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = encode_cyclic_features(df, cyclic_cols)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[standard_num_cols] = min_max_scaler.fit_transform(df[standard_num_cols])\n",
    "\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd856a7-92aa-4a5a-9de4-af35c0368760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating targets \n",
    "\n",
    "features_to_lag = {'unt_pre': 's(target)', 'rev_pre': 'r(target)'}\n",
    "\n",
    "flight_lags = [0]\n",
    "charge_lags = [0, -1, -2, -3, -4, -5, -6] \n",
    "\n",
    "df = lag_array(df, charge_lags, flight_lags, features_to_lag)\n",
    "df = df.drop(['unt_pre', 'rev_pre'], axis=1)\n",
    "optimize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a15c2ea-2a6f-4a03-8421-1e477f00d3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col in index_cols:\n",
    "    df[col] = original_df[col]\n",
    "df.set_index(index_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb11a7a0-e24c-49fe-a740-88e78ccc6bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining targets + features for each horizon\n",
    "\n",
    "all_target_cols = []\n",
    "for i in range(0, 7): \n",
    "    c_lag_str = f\"C{i}\" if i == 0 else f\"C-{i}\" \n",
    "    all_target_cols.append(f's(target)_{c_lag_str}F0')\n",
    "    all_target_cols.append(f'r(target)_{c_lag_str}F0')\n",
    "\n",
    "base_features = [col for col in df.columns if col not in all_target_cols]\n",
    "\n",
    "# Dictionaries to hold the data for each horizon\n",
    "X_data_dict = {}\n",
    "y_data_dict = {}\n",
    "\n",
    "for h in range(1, 8):\n",
    "    # Define target for this horizon\n",
    "    y_sales = df[f's(target)_C{1-h}F0']\n",
    "    y_revenue = df[f'r(target)_C{1-h}F0']\n",
    "    \n",
    "    # Define features for this horizon, dropping leaky lags\n",
    "    cols_to_drop_for_h = []\n",
    "    if h > 1:\n",
    "        for j in range(1, h):\n",
    "            cols_to_drop_for_h.append(f's_C{j}F0')\n",
    "            cols_to_drop_for_h.append(f'r_C{j}F0')\n",
    "\n",
    "    features_for_h = [col for col in base_features if col not in cols_to_drop_for_h]\n",
    "    X = df[features_for_h]\n",
    "    \n",
    "    # Store in dictionaries\n",
    "    X_data_dict[f'h{h}'] = X\n",
    "    y_data_dict[f'sales_h{h}'] = y_sales\n",
    "    y_data_dict[f'revenue_h{h}'] = y_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61d08732-301b-4e79-8828-fa582ddfc5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Master dictionary to store all scores\n",
    "all_horizon_scores = {}\n",
    "unique_dates = df.index.get_level_values('charge_dt').unique().sort_values()\n",
    "\n",
    "# Outer loop: Iterate through each forecast horizon\n",
    "for h in range(1, 8):    \n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=12, test_size=h)\n",
    "\n",
    "    X = X_data_dict[f'h{h}']\n",
    "    y_sales = y_data_dict[f'sales_h{h}']\n",
    "    y_revenue = y_data_dict[f'revenue_h{h}']\n",
    "    valid_sales_mask = (y_sales != -1)\n",
    "    valid_revenue_mask = (y_revenue != -1)\n",
    "    valid_mask = valid_sales_mask & valid_revenue_mask\n",
    "    X_clean = X[valid_mask]\n",
    "    y_sales_clean = y_sales[valid_mask]\n",
    "    y_revenue_clean = y_revenue[valid_mask]\n",
    "    unique_dates_clean = X_clean.index.get_level_values('charge_dt').unique().sort_values()\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axs = plt.subplots(4, 3, figsize=(16, 8), sharex=True)\n",
    "    fig.suptitle(f\"CV Folds for Horizon h={h}\", fontsize=16)\n",
    "\n",
    "    fold = 0\n",
    "    \n",
    "    sales_scores = []\n",
    "    revenue_scores = []\n",
    "    all_y_test_sales = []\n",
    "    all_y_pred_sales_series = []\n",
    "    all_y_test_revenue = []\n",
    "    all_y_pred_revenue_series = []\n",
    "\n",
    "    # Inner loop: Iterate through each CV fold\n",
    "    for train_index, val_index in tscv.split(unique_dates_clean):\n",
    "        train_dates = unique_dates_clean[train_index]\n",
    "        val_dates = unique_dates_clean[val_index]\n",
    "\n",
    "        train_mask = X_clean.index.get_level_values('charge_dt').isin(train_dates)\n",
    "        val_mask = X_clean.index.get_level_values('charge_dt').isin(val_dates)\n",
    "        \n",
    "        X_train, X_test = X_clean.loc[train_mask], X_clean.loc[val_mask]\n",
    "        \n",
    "        y_train_sales, y_test_sales = y_sales_clean.loc[train_mask], y_sales_clean.loc[val_mask]\n",
    "        y_train_revenue, y_test_revenue = y_revenue_clean.loc[train_mask], y_revenue_clean.loc[val_mask]\n",
    "\n",
    "        ax = axs[fold // 3, fold % 3]\n",
    "        y_train_sales.groupby('charge_dt').sum().plot(ax=ax, label='Train Sales')\n",
    "        y_test_sales.groupby('charge_dt').sum().plot(ax=ax, label='Val Sales')\n",
    "        ax.axvline(y_test_sales.index.get_level_values('charge_dt').min(), linestyle='--', color='black')\n",
    "        ax.set_title(f'Fold {fold+1}')\n",
    "        end_date = val_dates.max()\n",
    "        start_date = end_date - pd.DateOffset(months=6)\n",
    "        ax.set_xlim(start_date, end_date)\n",
    "        ax.set_title(f'Fold {fold+1}')\n",
    "\n",
    "        # --- Feature Scaling ---\n",
    "        #std_scaler = StandardScaler()\n",
    "        #current_normal_cols = [col for col in normal_num_cols if col in X_train.columns]\n",
    "        #X_train[current_normal_cols] = std_scaler.fit_transform(X_train[current_normal_cols])\n",
    "        #X_test[current_normal_cols] = std_scaler.transform(X_test[current_normal_cols])\n",
    "\n",
    "        # --- Train DUAL models ---\n",
    "        \n",
    "        # Sales Model\n",
    "        sales_model = lgb.LGBMRegressor(max_depth=5, learning_rate=0.5, n_estimators=2000, objective='huber', verbose=-1)\n",
    "        sales_model.fit(X_train, y_train_sales, eval_set=[(X_test, y_test_sales)], callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=0)])\n",
    "        \n",
    "        # Revenue Model\n",
    "        revenue_model = lgb.LGBMRegressor(max_depth=7, learning_rate=50, n_estimators=2000, objective='huber', verbose=-1)\n",
    "        revenue_model.fit(X_train, y_train_revenue, eval_set=[(X_test, y_test_revenue)], callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(period=0)])\n",
    "\n",
    "        # --- Predict and Score ---\n",
    "        \n",
    "        y_pred_sales = sales_model.predict(X_test)\n",
    "        y_pred_revenue = revenue_model.predict(X_test)\n",
    "        \n",
    "        sales_rmse = np.sqrt(mean_squared_error(y_test_sales, y_pred_sales))\n",
    "        revenue_rmse = np.sqrt(mean_squared_error(y_test_revenue, y_pred_revenue))\n",
    "        \n",
    "        sales_scores.append(sales_rmse)\n",
    "        revenue_scores.append(revenue_rmse)\n",
    "\n",
    "        y_pred_sales_series = pd.Series(y_pred_sales, index=X_test.index, name='sales_pred')\n",
    "        y_pred_revenue_series = pd.Series(y_pred_revenue, index=X_test.index, name='revenue_pred')\n",
    "        \n",
    "        all_y_test_sales.append(y_test_sales)\n",
    "        all_y_pred_sales_series.append(y_pred_sales_series)\n",
    "        \n",
    "        all_y_test_revenue.append(y_test_revenue)\n",
    "        all_y_pred_revenue_series.append(y_pred_revenue_series)\n",
    "        \n",
    "        fold += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Report and Store Scores for this Horizon ---\n",
    "    mean_sales_rmse = np.mean(sales_scores)\n",
    "    mean_revenue_rmse = np.mean(revenue_scores)\n",
    "    \n",
    "    print(f\"--- Results for h={h} ---\")\n",
    "    print(f\"Mean Sales RMSE: {mean_sales_rmse:.4f}\")\n",
    "    print(f\"Mean Revenue RMSE: {mean_revenue_rmse:.4f}\")\n",
    "    \n",
    "    all_horizon_scores[f'h{h}'] = {'sales_rmse_mean': mean_sales_rmse, 'revenue_rmse_mean': mean_revenue_rmse, 'sales_scores_fold': sales_scores, 'revenue_scores_fold': revenue_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "985fc4f0-2f6e-4e2a-8be9-fdeba7019a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'individual scores: {sales_scores}')\n",
    "print(f'individual scores: {revenue_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c740d8b0-e528-4e40-b358-80dc08673ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for h in range(1,8):\n",
    "    # 1. Sales Plot\n",
    "    full_y_test_sales = pd.concat(all_y_test_sales)\n",
    "    full_y_pred_sales = pd.concat(all_y_pred_sales_series)\n",
    "\n",
    "    y_test_sales_total = full_y_test_sales.groupby(level='charge_dt').sum()\n",
    "    y_pred_sales_total = full_y_pred_sales.groupby(level='charge_dt').sum()\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(y_test_sales_total.index, y_test_sales_total, label='Actual Daily Sales', marker='.', linestyle='-')\n",
    "    plt.plot(y_pred_sales_total.index, y_pred_sales_total, label='Predicted Daily Sales', marker='.', linestyle='--')\n",
    "    plt.title(f'Sales: Actual vs. Predicted Daily Totals (All Folds) - Horizon h={h}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Sales')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Revenue Plot\n",
    "    full_y_test_revenue = pd.concat(all_y_test_revenue)\n",
    "    full_y_pred_revenue = pd.concat(all_y_pred_revenue_series)\n",
    "\n",
    "    y_test_revenue_total = full_y_test_revenue.groupby(level='charge_dt').sum()\n",
    "    y_pred_revenue_total = full_y_pred_revenue.groupby(level='charge_dt').sum()\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(y_test_revenue_total.index, y_test_revenue_total, label='Actual Daily Revenue', marker='.', linestyle='-')\n",
    "    plt.plot(y_pred_revenue_total.index, y_pred_revenue_total, label='Predicted Daily Revenue', marker='.', linestyle='--')\n",
    "    plt.title(f'Revenue: Actual vs. Predicted Daily Totals (All Folds) - Horizon h={h}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Revenue')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apprenticeship Project (original volume forecast)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
