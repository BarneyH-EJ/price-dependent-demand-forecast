{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07636cc7-6bcf-4c8b-b39e-518d76a5e131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install linearmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead04c5c-3e81-44f9-b0bf-c9ed28f47c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6648ad41-90b4-4e51-a555-7a4ef04e47b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import weekofyear\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Data\n",
    "\n",
    "sales_history = spark.read.table('data_experience_commercial.cbt_1423_rtsuite.master_uat').select('flightkey', F.col('charge_dt').cast('date'), 'unt_net', 'chargeproduct', 'dtg')\n",
    "dimensions_history = spark.read.table('data_experience_commercial.cbt_0923_segmentfinder.dimensions_history').select('flightkey', 'route', 'onsale_dt', 'ty_capacity', F.col('flight_dt').cast('date'))\n",
    "\n",
    "# Filter Data\n",
    "\n",
    "filtered_sales = sales_history.filter((F.col('chargeproduct')=='Ticket') & (F.col('dtg')>=0)) # ticket only + eliminate covid, ss, and not yet flown\n",
    "filtered_dimensions = dimensions_history.filter((F.datediff(F.col('flight_dt'), F.col('onsale_dt')) >= 168) & (F.col('flight_dt') >= '2024-07-01') & (F.col('flight_dt') <= '2024-09-30')) # eliminate late top-ups, filter for LY flight dates \n",
    "\n",
    "# Preprocessing - filling missing charge dates\n",
    "\n",
    "dsh = filtered_sales.join(filtered_dimensions, on='flightkey', how='inner') # join tables\n",
    "dshsmooth = dsh.groupby('flightkey','charge_dt').agg(F.sum('unt_net').alias('unt_net'), F.first('onsale_dt').alias('onsale_dt'), F.first('flight_dt').alias('flight_dt')) # aggregate into daily flight sales\n",
    "date_range = dshsmooth.groupBy('flightkey').agg(F.min('onsale_dt').alias('start_date'), F.least(F.first('flight_dt'), F.lit(datetime.now().date())).alias('end_date')) # define flight onsale period\n",
    "index = date_range.withColumn('charge_dt_ts', F.explode(F.sequence(F.col('start_date'), F.col('end_date')))).withColumn('charge_dt', F.col('charge_dt_ts').cast('date')) # create index of dates between onsale and flight date\n",
    "dshjoin = index.join(dshsmooth, on=['flightkey', 'charge_dt'], how='left').drop('start_date', 'end_date','onsale_dt','flight_dt','charge_dt_ts').fillna(0) # join index with daily sales\n",
    "window_spec = Window.partitionBy('flightkey').orderBy(F.col('charge_dt')) # create window for rolling pax sum\n",
    "dsh_pax = dshjoin.withColumn('pax_net', F.sum('unt_net').over(window_spec)) # calculate current pax sum - currently neglects cancellations\n",
    "\n",
    "# Preprocessing - creating LF by dtg progress remaining curves for each sector week\n",
    "\n",
    "final_dsh = dsh_pax.join(filtered_dimensions, on='flightkey', how='left').drop('unt_net') # join daily sales with dimensions\n",
    "curves = final_dsh.withColumn('total_booking_days', F.datediff(F.col('flight_dt'), F.col('onsale_dt'))) # calculate on sale period length in days\n",
    "curves = curves.withColumn('dtg', F.datediff(F.col('flight_dt'), F.col('charge_dt'))) # calculate dtg\n",
    "normal_curves = curves.withColumn('dtg_pr', F.col('dtg') / F.col('total_booking_days')) # express dtg progress remaining as dtg as a fraction of total booking days\n",
    "normal_curve_buckets = normal_curves.withColumn('dtg_bucket', (F.floor(F.col('dtg_pr') * 100)).cast('int'))  # split dtg_pr into percentile buckets\n",
    "aggregated_normal_curves = normal_curve_buckets.groupby('route', 'dtg_bucket').agg(F.sum('ty_capacity').alias('ty_capacity'), F.sum('pax_net').alias('pax_net')).orderBy('dtg_bucket') # aggregate by sector week and dtg bucket\n",
    "df = aggregated_normal_curves.withColumn('load_factor', F.col('pax_net')/F.col('ty_capacity')).drop('ty_capacity', 'pax_net').toPandas() # create pandas dataframe of LF by dtg progress remaining by sector week\n",
    "\n",
    "# storing original dataframe\n",
    "\n",
    "df.info()\n",
    "df_original = df.copy()\n",
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a3652f-ba3d-48fa-a986-07f5d033a842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pivot = df.pivot_table(index='route', columns='dtg_bucket', values='load_factor', aggfunc='mean').fillna(0)\n",
    "df_cluster = df_pivot[df_pivot[1] > 0]\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23bcf9aa-dcb1-43fc-849f-97fdf44e95d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing all routes that didnt have final LF > 60%\n",
    "\n",
    "df_sold = df_cluster[df_cluster[1] > 0.6]\n",
    "df_sold.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2abe9866-5a78-4c83-aab0-47549c9a0cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Expressing Load Factor as a function of final LF\n",
    "df_sold = df_sold.div(df_sold[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e3461e-c969-471b-9247-28b7f20cb4b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# initial LF set to 0\n",
    "\n",
    "row_min = df_sold.min(axis=1)\n",
    "row_max = df_sold.max(axis=1)\n",
    "\n",
    "df_sold = df_sold.subtract(row_min, axis=0).divide(row_max - row_min, axis=0)\n",
    "df_sold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9dfcbb8-b7a4-422f-80af-7629d16da253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "k_range = range(2, 11)\n",
    "wcss = []\n",
    "for i in k_range: \n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', n_init=10)\n",
    "    kmeans.fit(df_sold)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS (Inertia)')\n",
    "plt.xticks(k_range) \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac8739c-1561-491e-b455-79434a30d107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sold_original = df_sold.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccb7777-d42c-4d4a-b001-388e67d7ee23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sold = df_sold_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c3a1a9-9d36-4127-a91c-eab77270fa9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', n_init=10)\n",
    "kmeans.fit(df_sold)\n",
    "df_sold['cluster_label'] = kmeans.labels_   \n",
    "curves_clusters = df_sold.groupby('cluster_label').mean()\n",
    "\n",
    "curves_clusters.T.plot(figsize=(20, 8))\n",
    "\n",
    "plt.xlabel('% DTG remaining')\n",
    "plt.ylabel('LF / LF_final')\n",
    "plt.title('Isolated booking curve shape clustering \"Banana split\"')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title=' # Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8440ab37-a58d-4a93-ad42-db31e2fce230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col40 = df_sold.groupby('cluster_label')[40].mean()\n",
    "sorted_clusters = col40.sort_values()\n",
    "\n",
    "auto_mapping = {}\n",
    "for new_rank, old_label in enumerate(sorted_clusters.index, start=1):\n",
    "    auto_mapping[old_label] = new_rank\n",
    "\n",
    "df_sold['new_cluster_label'] = df_sold['cluster_label'].map(auto_mapping)\n",
    "\n",
    "new_curves_clusters = df_sold.drop(columns=['cluster_label']).groupby('new_cluster_label').mean()\n",
    "\n",
    "new_curves_clusters.T.plot(figsize=(20, 8))\n",
    "\n",
    "plt.xlabel('% DTG remaining')\n",
    "plt.ylabel('LF / LF_final')\n",
    "plt.title('Isolated booking curve shape clustering \"Banana split\"')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title=' # Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540397f8-7b96-450c-bce0-ffdd9d2726e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sold['cluster_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816f8ba9-3d8f-494b-baaa-b823d2be7a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY25Q3 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY25Q3.csv')\n",
    "FY25Q3.sort_values(by='flight_time', inplace=True)\n",
    "FY25Q3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "714185ef-9ae8-4b19-b508-d73109745af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY25Q4 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY25Q4.csv')\n",
    "FY25Q4.sort_values(by='flight_time', inplace=True)\n",
    "FY25Q4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b58c0d-032f-437e-923a-9fc84e715680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_internal = pd.concat([FY25Q3, FY25Q4], ignore_index=True)\n",
    "df_internal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "810cc1dd-4ab4-4405-a0e7-357c72250203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cluster_df = pd.merge(df_internal, df_sold['new_cluster_label'], on='route', how='left')\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cad463-ed65-41f8-9739-64156e70f760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_external = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/wealth statistics.csv')\n",
    "df_external.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4887eafa-c515-44a2-a660-6a2d5b7b6a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_initial = pd.merge(cluster_df, df_external[['Airport','National (Nominal) GDP Per Capita ($) (worldbank 2024)', 'National (PPP) GDP Per Capita ($) (worldbank 2024)', 'Local GDP Per Capita ($)']], left_on='base', right_on='Airport', how='left').rename(columns={'National (Nominal) GDP Per Capita ($) (worldbank 2024)': 'Ngdppc_base', 'National (PPP) GDP Per Capita ($) (worldbank 2024)': 'Pgdppc_base', 'Local GDP Per Capita ($)': 'Lgdppc_base'})\n",
    "df = pd.merge(df_initial, df_external[['Airport','National (Nominal) GDP Per Capita ($) (worldbank 2024)', 'National (PPP) GDP Per Capita ($) (worldbank 2024)', 'Local GDP Per Capita ($)']], left_on='dest', right_on='Airport', how='left').rename(columns={'National (Nominal) GDP Per Capita ($) (worldbank 2024)': 'Ngdppc_dest', 'National (PPP) GDP Per Capita ($) (worldbank 2024)': 'Pgdppc_dest', 'Local GDP Per Capita ($)': 'Lgdppc_dest'})\n",
    "\n",
    "df['local_wealth'] = ((df['prop_from_base'] * df['Lgdppc_base']) + (df['prop_from_dest'] * df['Lgdppc_dest']))\n",
    "df['national_wealth_nominal'] = ((df['prop_from_base'] * df['Ngdppc_base']) + (df['prop_from_dest'] * df['Ngdppc_dest']))\n",
    "df['national_wealth_ppp'] = ((df['prop_from_base'] * df['Pgdppc_base']) + (df['prop_from_dest'] * df['Pgdppc_dest']))\n",
    "df = df.drop(['base', 'dest', 'prop_from_base', 'prop_from_dest', 'Airport_x','Airport_y','Ngdppc_base', 'Ngdppc_dest', 'Pgdppc_base', 'Pgdppc_dest', 'Lgdppc_base', 'Lgdppc_dest'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9669c478-b793-4282-b814-7b8d201f7286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb2c48e-2ea0-4bfa-bfce-1b9279194e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df = df[df['cumulative_sales'] >= 0]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66f9b55-9dfd-445b-984e-9483c89dc477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "uk_holidays = holidays.UK(years=range(2024, 2026))\n",
    "holidays_df = pd.DataFrame([(date, name) for date, name in uk_holidays.items()], columns=['ds', 'holiday'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e778849-fd07-44ba-bb98-bf18dd494fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "additional_holidays = pd.DataFrame([\n",
    "    {'ds': '2024-04-01', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2024-08-26', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2025-04-21', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2025-08-25', 'holiday': 'Summer Bank Holiday'},\n",
    "])\n",
    "holidays_df = pd.concat([holidays_df, additional_holidays], ignore_index=True)\n",
    "holidays_df.drop_duplicates(inplace=True)\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df.reset_index(drop=True, inplace=True)\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f21761-5124-4153-8a5a-b85a64a13ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"charge_dt\"] = pd.to_datetime(df[\"charge_dt\"])\n",
    "df[\"flight_dt\"] = pd.to_datetime(df[\"flight_dt\"])\n",
    "holidays_df[\"ds\"] = pd.to_datetime(holidays_df[\"ds\"])\n",
    "\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'charge_dt', 'holiday': 'charge_dt_holiday'}), how='left', on='charge_dt')\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'flight_dt', 'holiday': 'flight_dt_holiday'}), how='left', on='flight_dt')\n",
    "df['is_charge_date_holiday'] = df['charge_dt_holiday'].notnull().astype(int)\n",
    "df['is_flight_date_holiday'] = df['flight_dt_holiday'].notnull().astype(int)\n",
    "df.drop(['charge_dt_holiday', 'flight_dt_holiday'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba123cd0-219f-47c6-b9c6-d3a3a2097fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def profile(x100, x60, x20, x00, dtg, capacity):\n",
    "\n",
    "    x1 = (x60 - x100)/(x00 - x100)\n",
    "    x2 = (x20 - x100)/(x00 - x100)\n",
    "    b = np.log(np.log((1-(x1**4))/0.6)/np.log((1-(x2**4))/0.2))/np.log(x1/x2) \n",
    "    a = np.log((1-(x1**4))/0.6)/(x1**b) \n",
    "\n",
    "    x_norm = (dtg - x100)/(x00 - x100)\n",
    "    y_val = (np.exp(-a * (x_norm**b)))*(1-(x_norm**4))\n",
    "\n",
    "    return y_val*capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6c5973-db02-4001-a36d-a4056d13157e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['profile_tgt'] = profile(df['x100'], df['x60'], df['x20'], df['x00'], df['dtg'], df['ty_capacity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8531c81c-db31-4aee-a26d-c65cab05c06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['distance_to_profile'] = df['cumulative_sales'] - df['profile_tgt'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572e72ab-ccf8-4e8a-a73d-e7353c7b7b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['profile_distance_bucket'] = pd.cut(df['distance_to_profile'], bins=[-np.inf, -50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50, np.inf], labels=['-50+', '-40 to -50', '-30 to -40', '-20 to -30', '-10 to -20', '-10 to 0', '0 to 10', '10 to 20', '20 to 30', '30 to 40', '40 to 50', '50+'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab11fa7-e4ee-4ac3-a756-f2eaeb1a6bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df, x='profile_distance_bucket', y='treatment_delta')\n",
    "plt.title('Treatment Delta by Profile Distance Bucket (unt_net)')\n",
    "plt.xlabel('Profile Distance Bucket')\n",
    "plt.ylabel('Treatment Delta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fba09ff-30e7-4d06-bf39-eda0ee477341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['Z_instrument'] = df['distance_to_profile']\n",
    "df['control_ROS'] = df['log_sales_pre']\n",
    "df['control_loadfactor'] = np.log(df['Loadfactor'] + 0.01)\n",
    "df['control_dtg'] = df['dtg']\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bcfe98a-5208-4b1b-b129-8ac2b757daf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_first_stage = df[['Z_instrument', 'control_ROS', 'control_loadfactor', 'control_dtg']]\n",
    "y_first_stage = df['treatment_delta']\n",
    "\n",
    "X_first_stage = sm.add_constant(X_first_stage)\n",
    "\n",
    "model_first_stage = sm.OLS(y_first_stage, X_first_stage).fit()\n",
    "\n",
    "print(model_first_stage.summary())\n",
    "\n",
    "f_test = model_first_stage.f_test(\"Z_instrument = 0\")\n",
    "f_stat = f_test.fvalue\n",
    "\n",
    "print(f\"Instrument F-Statistic: {f_stat}\")\n",
    "\n",
    "if f_stat > 10:\n",
    "    print(\"PASS: Instrument is Strong. (F > 10)\")\n",
    "else:\n",
    "    print(\"FAIL: Instrument is Weak. (F < 10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e00cbcd-6a3d-4c24-a539-b541358138fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cyclic_cols = ['flight_dow', 'charge_dow', 'flight_dom', 'charge_dom', 'flight_mth', 'charge_mth']\n",
    "\n",
    "def encode_cyclic_features(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        max_val = df[col].max()\n",
    "        df[col + '_sin'] = (np.sin(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df[col + '_cos'] = (np.cos(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = encode_cyclic_features(df, cyclic_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec4d9bd-0bcb-4b89-82d7-0c6c0ae6ae54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.sort_values('flight_time')\n",
    "train_size = int(len(df) * 0.8)\n",
    "\n",
    "df_train = df.iloc[:train_size]\n",
    "df_test = df.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febc6ae3-dd96-4d4a-8012-b53d741f5482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "cols_to_scale = ['dtg','total_optionality_score','combined_bp','sale_length','lag_sales_7','lag_sales_14','lag_sales_21','lag_sales_28','Loadfactor','log_sales_pre','sale_period_progress','new_cluster_label','time_quality_score','ty_capacity','local_wealth']\n",
    "\n",
    "preprocessor = ColumnTransformer([('scaler', StandardScaler(), cols_to_scale)], remainder='passthrough', verbose_feature_names_out=False )\n",
    "\n",
    "pipeline = Pipeline([('preprocessor', preprocessor)])\n",
    "\n",
    "df_train_scaled = pipeline.fit_transform(df_train)\n",
    "df_test_scaled = pipeline.transform(df_test)\n",
    "df_train_scaled['dtg_sq_scaled'] = df_train_scaled['dtg'] ** 2\n",
    "df_test_scaled['dtg_sq_scaled'] = df_test_scaled['dtg'] ** 2\n",
    "df_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec30ed1f-960a-4039-9905-0fe84b10aad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "Y = df_train_scaled['outcome_delta'].values\n",
    "T = df_train_scaled['treatment_delta'].values\n",
    "Z = df_train_scaled['Z_instrument'].values\n",
    "\n",
    "W = df_train_scaled[['flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','dtg','dtg_sq_scaled','lag_sales_7','lag_sales_14','lag_sales_21','lag_sales_28','Loadfactor','log_sales_pre','sale_length','sale_period_progress','new_cluster_label','time_quality_score','ty_capacity','is_charge_date_holiday','is_flight_date_holiday']].values\n",
    "\n",
    "X = df_train_scaled[['dtg','dtg_sq_scaled','total_optionality_score','combined_bp','local_wealth']].values\n",
    "X_names = ['dtg','dtg_sq_scaled','total_optionality_score','combined_bp','local_wealth']\n",
    "\n",
    "print(f\"Running Manual DML on {len(Y)} rows...\")\n",
    "print(\"Step 1: Cleaning Data (Orthogonalization)...\")\n",
    "\n",
    "def get_residuals(target, controls):\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(controls, target)\n",
    "    return target - model.predict(controls)\n",
    "\n",
    "y_res = get_residuals(Y, W)\n",
    "t_res = get_residuals(T, W)\n",
    "z_res = get_residuals(Z, W)\n",
    "\n",
    "print(\"Step 2: Creating Interactions...\")\n",
    "\n",
    "T_interaction = t_res.reshape(-1, 1) * X\n",
    "T_final = np.column_stack([t_res, T_interaction])\n",
    "\n",
    "Z_interaction = z_res.reshape(-1, 1) * X\n",
    "Z_final = np.column_stack([z_res, Z_interaction])\n",
    "\n",
    "print(\"Step 3: Running Final IV Regression...\")\n",
    "\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "interaction_names = [f\"Price_x_{col}\" for col in X_names]\n",
    "exog_names = ['Price'] + interaction_names\n",
    "\n",
    "df_Y_res = pd.DataFrame(y_res, columns=['Sales_Res'])\n",
    "df_T_final = pd.DataFrame(T_final, columns=exog_names)\n",
    "df_Z_final = pd.DataFrame(Z_final, columns=[f\"Instr_{i}\" for i in range(Z_final.shape[1])])\n",
    "\n",
    "model = IV2SLS(dependent=df_Y_res, exog=None, endog=df_T_final, instruments=df_Z_final)\n",
    "\n",
    "results = model.fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e983aff-0afb-4c20-8af2-5358fc0d6bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from linearmodels.iv import IV2SLS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. PREDICT ELASTICITY ON TEST SET\n",
    "# ---------------------------------------------------------\n",
    "# We use the formula from your trained model's coefficients.\n",
    "# Replace these values with the EXACT outputs from your last run.\n",
    "BETA_INTERCEPT = -1.3782\n",
    "BETA_DTG = 0.1977\n",
    "BETA_DTG2 = \n",
    "BETA_OPT = -0.0848\n",
    "BETA_BP = 0.2534\n",
    "BETA_WEALTH = 0.1375\n",
    "\n",
    "def predict_elasticity(row):\n",
    "    e = BETA_INTERCEPT\n",
    "    e += BETA_DTG * row['dtg']                 # Scaled DTG\n",
    "    e += BETA_BP * row['combined_bp']      # Scaled BP\n",
    "    e += BETA_WEALTH * row['local_wealth'] # Scaled Wealth\n",
    "    e += BETA_OPT * row['total_optionality_score'] # Scaled Opt\n",
    "    return e\n",
    "\n",
    "# Apply to Test Set (Make sure it's scaled!)\n",
    "df_test_scaled['predicted_elasticity'] = df_test_scaled.apply(predict_elasticity, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. CREATE BUCKETS (QUINTILES)\n",
    "# ---------------------------------------------------------\n",
    "# Sort by predicted elasticity and split into 5 groups\n",
    "df_test_scaled['elasticity_bin'] = pd.qcut(df_test_scaled['predicted_elasticity'], 5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Buckets created. Calculating ACTUAL elasticity per bucket...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. VERIFY EACH BUCKET (THE TRUTH TEST)\n",
    "# ---------------------------------------------------------\n",
    "# We run a mini-IV regression for each bin to see the REAL slope.\n",
    "\n",
    "bucket_results = []\n",
    "bucket_labels = []\n",
    "\n",
    "for bin_num in [1, 2, 3, 4, 5]:\n",
    "    # Filter data for this bucket\n",
    "    subset = df_test_scaled[df_test_scaled['elasticity_bin'] == bin_num]\n",
    "    \n",
    "    # Run simple IV: Sales ~ Price (Instrumented by Z)\n",
    "    # We use the same 'get_residuals' logic or just raw IV if sample is large enough\n",
    "    # Here we use raw IV for simplicity of the test check\n",
    "    iv_mod = IV2SLS(\n",
    "        dependent=subset['outcome_delta'],\n",
    "        exog=None,\n",
    "        endog=subset['treatment_delta'],\n",
    "        instruments=subset['Z_instrument']\n",
    "    ).fit()\n",
    "    \n",
    "    # The coefficient of 'treatment_delta' is the ACTUAL elasticity of this group\n",
    "    real_elasticity = iv_mod.params['treatment_delta']\n",
    "    bucket_results.append(real_elasticity)\n",
    "    bucket_labels.append(f\"Bin {bin_num}\")\n",
    "    \n",
    "    print(f\"Bin {bin_num} (Model pred: {subset['predicted_elasticity'].mean():.2f}) -> Actual Slope: {real_elasticity:.2f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VISUALIZE THE SUCCESS\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bucket_labels, bucket_results, color='skyblue', edgecolor='black')\n",
    "plt.ylabel('Actual Observed Elasticity (IV Slope)')\n",
    "plt.xlabel('Model Predicted Sensitivity (Bin 1 = Most Elastic)')\n",
    "plt.title('Model Validation: Did we correctly sort the flights?')\n",
    "plt.axhline(0, color='black', linewidth=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apprenticeship Project (elasticity estimate)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
