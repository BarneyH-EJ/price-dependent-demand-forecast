{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18f9405-2ecd-486d-84a2-458e95699be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deriving Weights of Simple Linear Elasticity Model using ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07636cc7-6bcf-4c8b-b39e-518d76a5e131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install linearmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead04c5c-3e81-44f9-b0bf-c9ed28f47c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86aef8f2-74dc-46de-a71d-abf5a7c0aca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Finidng Routetype Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6648ad41-90b4-4e51-a555-7a4ef04e47b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import weekofyear\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import substring, col\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Data\n",
    "\n",
    "sales_history = spark.read.table('data_experience_commercial.cbt_1423_rtsuite.master_uat').select('flightkey', F.col('charge_dt').cast('date'), 'unt_net', 'chargeproduct', 'dtg')\n",
    "dimensions_history_ = spark.read.table('data_experience_commercial.cbt_0923_segmentfinder.dimensions_history').select('flightkey', 'onsale_dt', 'ty_capacity', F.col('flight_dt').cast('date'))\n",
    "dimensions_history = dimensions_history_.withColumn('sectormonth', F.concat(substring(col('flightkey'), 9, 6), substring(col('flightkey'), 5, 2)))\n",
    "\n",
    "# Filter Data\n",
    "\n",
    "filtered_sales = sales_history.filter((F.col('chargeproduct')=='Ticket') & (F.col('dtg')>=0)) # ticket only + eliminate covid, ss, and not yet flown\n",
    "filtered_dimensions = dimensions_history.filter((F.datediff(F.col('flight_dt'), F.col('onsale_dt')) >= 168) & (F.col('flight_dt') >= '2023-04-01') & (F.col('flight_dt') <= '2024-03-31')) # eliminate late top-ups, filter for LY flight dates \n",
    "\n",
    "# Preprocessing - filling missing charge dates\n",
    "\n",
    "dsh = filtered_sales.join(filtered_dimensions, on='flightkey', how='inner') # join tables\n",
    "dshsmooth = dsh.groupby('flightkey','charge_dt').agg(F.sum('unt_net').alias('unt_net'), F.first('onsale_dt').alias('onsale_dt'), F.first('flight_dt').alias('flight_dt')) # aggregate into daily flight sales\n",
    "date_range = dshsmooth.groupBy('flightkey').agg(F.min('onsale_dt').alias('start_date'), F.least(F.first('flight_dt'), F.lit(datetime.now().date())).alias('end_date')) # define flight onsale period\n",
    "index = date_range.withColumn('charge_dt_ts', F.explode(F.sequence(F.col('start_date'), F.col('end_date')))).withColumn('charge_dt', F.col('charge_dt_ts').cast('date')) # create index of dates between onsale and flight date\n",
    "dshjoin = index.join(dshsmooth, on=['flightkey', 'charge_dt'], how='left').drop('start_date', 'end_date','onsale_dt','flight_dt','charge_dt_ts').fillna(0) # join index with daily sales\n",
    "window_spec = Window.partitionBy('flightkey').orderBy(F.col('charge_dt')) # create window for rolling pax sum\n",
    "dsh_pax = dshjoin.withColumn('pax_net', F.sum('unt_net').over(window_spec)) # calculate current pax sum - currently neglects cancellations\n",
    "\n",
    "# Preprocessing - creating LF by dtg progress remaining curves for each sector week\n",
    "\n",
    "final_dsh = dsh_pax.join(filtered_dimensions, on='flightkey', how='left').drop('unt_net') # join daily sales with dimensions\n",
    "curves = final_dsh.withColumn('total_booking_days', F.datediff(F.col('flight_dt'), F.col('onsale_dt'))) # calculate on sale period length in days\n",
    "curves = curves.withColumn('dtg', F.datediff(F.col('flight_dt'), F.col('charge_dt'))) # calculate dtg\n",
    "normal_curves = curves.withColumn('dtg_pr', F.col('dtg') / F.col('total_booking_days')) # express dtg progress remaining as dtg as a fraction of total booking days\n",
    "normal_curve_buckets = normal_curves.withColumn('dtg_bucket', (F.floor(F.col('dtg_pr') * 100)).cast('int'))  # split dtg_pr into percentile buckets\n",
    "aggregated_normal_curves = normal_curve_buckets.groupby('sectormonth', 'dtg_bucket').agg(F.sum('ty_capacity').alias('ty_capacity'), F.sum('pax_net').alias('pax_net')).orderBy('dtg_bucket') # aggregate by sector week and dtg bucket\n",
    "df = aggregated_normal_curves.withColumn('load_factor', F.col('pax_net')/F.col('ty_capacity')).drop('ty_capacity', 'pax_net').toPandas() # create pandas dataframe of LF by dtg progress remaining by sector week\n",
    "\n",
    "# storing original dataframe\n",
    "\n",
    "df.info()\n",
    "df_original = df.copy()\n",
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a3652f-ba3d-48fa-a986-07f5d033a842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pivot = df.pivot_table(index='sectormonth', columns='dtg_bucket', values='load_factor', aggfunc='mean').fillna(0)\n",
    "df_cluster = df_pivot[df_pivot[1] > 0]\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23bcf9aa-dcb1-43fc-849f-97fdf44e95d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing all routes that didnt have final LF > 60%\n",
    "\n",
    "df_sold = df_cluster[df_cluster[1] > 0.6]\n",
    "df_sold.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2abe9866-5a78-4c83-aab0-47549c9a0cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Expressing Load Factor as a function of final LF\n",
    "df_sold = df_sold.div(df_sold[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e3461e-c969-471b-9247-28b7f20cb4b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# initial LF set to 0\n",
    "\n",
    "row_min = df_sold.min(axis=1)\n",
    "row_max = df_sold.max(axis=1)\n",
    "\n",
    "df_sold = df_sold.subtract(row_min, axis=0).divide(row_max - row_min, axis=0)\n",
    "df_sold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9dfcbb8-b7a4-422f-80af-7629d16da253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "k_range = range(2, 11)\n",
    "wcss = []\n",
    "for i in k_range: \n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', n_init=10)\n",
    "    kmeans.fit(df_sold)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS (Inertia)')\n",
    "plt.xticks(k_range) \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac8739c-1561-491e-b455-79434a30d107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sold_original = df_sold.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccb7777-d42c-4d4a-b001-388e67d7ee23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sold = df_sold_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c3a1a9-9d36-4127-a91c-eab77270fa9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', n_init=10)\n",
    "kmeans.fit(df_sold)\n",
    "df_sold['cluster_label'] = kmeans.labels_   \n",
    "curves_clusters = df_sold.groupby('cluster_label').mean()\n",
    "\n",
    "curves_clusters.T.plot(figsize=(20, 8))\n",
    "\n",
    "plt.xlabel('% DTG remaining')\n",
    "plt.ylabel('LF / LF_final')\n",
    "plt.title('Isolated booking curve shape clustering \"Banana split\"')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title=' # Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8440ab37-a58d-4a93-ad42-db31e2fce230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col40 = df_sold.groupby('cluster_label')[40].mean()\n",
    "sorted_clusters = col40.sort_values()\n",
    "\n",
    "auto_mapping = {}\n",
    "for new_rank, old_label in enumerate(sorted_clusters.index, start=1):\n",
    "    auto_mapping[old_label] = new_rank\n",
    "\n",
    "df_sold['new_cluster_label'] = df_sold['cluster_label'].map(auto_mapping)\n",
    "\n",
    "new_curves_clusters = df_sold.drop(columns=['cluster_label']).groupby('new_cluster_label').mean()\n",
    "\n",
    "new_curves_clusters.T.plot(figsize=(20, 8))\n",
    "\n",
    "plt.xlabel('% DTG remaining')\n",
    "plt.ylabel('LF / LF_final')\n",
    "plt.title('Isolated booking curve shape clustering \"Banana split\"')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(title=' # Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540397f8-7b96-450c-bce0-ffdd9d2726e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sold['cluster_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bf576cf-94c7-429e-907b-ff1d009b951f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY24Q3 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY24Q3.csv')\n",
    "FY24Q3.sort_values(by='flight_time', inplace=True)\n",
    "FY24Q3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8974d9c2-3143-440a-b504-7c8429c8c740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY24Q4 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY24Q4.csv')\n",
    "FY24Q4.sort_values(by='flight_time', inplace=True)\n",
    "FY24Q4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf562a90-a36b-4c2e-97b9-e9a4a90c6221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY25Q1 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY25Q1.csv')\n",
    "FY25Q1.sort_values(by='flight_time', inplace=True)\n",
    "FY25Q1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335b8631-464d-47cd-9ec3-5b13eac6f02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY25Q2 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY25Q2.csv')\n",
    "FY25Q2.sort_values(by='flight_time', inplace=True)\n",
    "FY25Q2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a7fe87-5669-4c3f-a91d-39bfde707073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY25Q3_1 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY25Q3DTG+100.csv')\n",
    "FY25Q3_1.sort_values(by='flight_time', inplace=True)\n",
    "FY25Q3_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9778fc-2244-46a8-9cc9-057a045f80aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY25Q3_2 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY25Q3DTG-100.csv')\n",
    "FY25Q3_2.sort_values(by='flight_time', inplace=True)\n",
    "FY25Q3_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6407897-b84c-4357-bd0e-3664a3424151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY25Q4_1 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY25Q4DTG+100.csv')\n",
    "FY25Q4_1.sort_values(by='flight_time', inplace=True)\n",
    "FY25Q4_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec8f690-cb89-4e62-8dd2-acbdcbc0842d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY25Q4_2 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY25Q4DTG-100.csv')\n",
    "FY25Q4_2.sort_values(by='flight_time', inplace=True)\n",
    "FY25Q4_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95d2f739-5d8b-4e42-afb2-431159f94779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FY26Q1 = pd.read_csv('/Workspace/Users/barney.hodge@easyjet.com/FY26Q1.csv')\n",
    "FY26Q1.sort_values(by='flight_time', inplace=True)\n",
    "FY26Q1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b58c0d-032f-437e-923a-9fc84e715680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_internal = pd.concat([FY24Q3, FY24Q4, FY25Q1, FY25Q2, FY25Q3_1, FY25Q3_2, FY25Q4_1, FY25Q4_2, FY26Q1], ignore_index=True)\n",
    "df_internal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e68097-1152-47f0-9000-71116d45eac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def optimize_df(df):\n",
    "\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Initial memory usage: {start_mem:.2f} MB\")\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        if '_dt' in col:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "        elif 'flightkey' in col or 'region' in col or 'routetype' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        elif 'float' in str(df[col].dtype):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "            \n",
    "        elif 'int' in str(df[col].dtype):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(f\"Final memory usage: {end_mem:.2f} MB ({reduction:.2f}% reduction)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "optimize_df(df_internal)\n",
    "df_internal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0379f25-aaf9-43e4-bf81-98b7872496b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_internal['sectormonth'] = (df_internal['flightkey'].astype(str).str[8:14] + df_internal['flightkey'].astype(str).str[4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "810cc1dd-4ab4-4405-a0e7-357c72250203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_initial = pd.merge(df_internal, df_sold['new_cluster_label'], on='sectormonth', how='left')\n",
    "df_initial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe394f2-2c8c-4b81-b40e-4cb9dcf4668f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_initial.drop(columns=['flight_time','route','sectormonth','total_optionality_score','combined_bp','combined_bp_DoW','base','dest','prop_from_base','prop_from_dest','time_quality_score'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba2207d-c08e-4576-977b-c6952710f3f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimize_df(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb2c48e-2ea0-4bfa-bfce-1b9279194e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['cumulative_sales'] >= 0]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb3d6c2-5791-4569-bf1e-23d64d615ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66f9b55-9dfd-445b-984e-9483c89dc477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "uk_holidays = holidays.UK(years=range(2024, 2026))\n",
    "holidays_df = pd.DataFrame([(date, name) for date, name in uk_holidays.items()], columns=['ds', 'holiday'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e778849-fd07-44ba-bb98-bf18dd494fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "additional_holidays = pd.DataFrame([\n",
    "    {'ds': '2024-04-01', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2024-08-26', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2025-04-21', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2025-08-25', 'holiday': 'Summer Bank Holiday'},\n",
    "])\n",
    "holidays_df = pd.concat([holidays_df, additional_holidays], ignore_index=True)\n",
    "holidays_df.drop_duplicates(inplace=True)\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df.reset_index(drop=True, inplace=True)\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f21761-5124-4153-8a5a-b85a64a13ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"charge_dt\"] = pd.to_datetime(df[\"charge_dt\"])\n",
    "df[\"flight_dt\"] = pd.to_datetime(df[\"flight_dt\"])\n",
    "holidays_df[\"ds\"] = pd.to_datetime(holidays_df[\"ds\"])\n",
    "\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'charge_dt', 'holiday': 'charge_dt_holiday'}), how='left', on='charge_dt')\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'flight_dt', 'holiday': 'flight_dt_holiday'}), how='left', on='flight_dt')\n",
    "df['is_charge_date_holiday'] = df['charge_dt_holiday'].notnull().astype(int)\n",
    "df['is_flight_date_holiday'] = df['flight_dt_holiday'].notnull().astype(int)\n",
    "df.drop(['charge_dt_holiday', 'flight_dt_holiday'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6985c4-fd5d-4fc3-9d6f-339590e67895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['is_charge_date_weekday'] = (df['charge_dt'].dt.dayofweek < 5).astype(int)\n",
    "df['is_flight_date_weekday'] = (df['flight_dt'].dt.dayofweek < 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba123cd0-219f-47c6-b9c6-d3a3a2097fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "\n",
    "def profile(x100, x60, x20, x00, dtg, capacity):\n",
    "\n",
    "    x1 = (x60 - x100)/(x00 - x100)\n",
    "    x2 = (x20 - x100)/(x00 - x100)\n",
    "    b = np.log(np.log((1-(x1**4))/0.6)/np.log((1-(x2**4))/0.2))/np.log(x1/x2) \n",
    "    a = np.log((1-(x1**4))/0.6)/(x1**b) \n",
    "\n",
    "    x_norm = (dtg - x100)/(x00 - x100)\n",
    "    y_val = (np.exp(-a * (x_norm**b)))*(1-(x_norm**4))\n",
    "\n",
    "    return y_val*capacity'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6c5973-db02-4001-a36d-a4056d13157e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df['profile_tgt'] = profile(df['x100'], df['x60'], df['x20'], df['x00'], df['dtg'], df['ty_capacity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96731e5f-0926-4616-8ed7-6c6671658769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df['profile_tgt'] = df['prof_LF']*df['ty_lid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572e72ab-ccf8-4e8a-a73d-e7353c7b7b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['profile_distance_bucket'] = pd.cut(df['Z_instrument'], bins=[-np.inf, -32, -16, -8, -4, -2, 0, 2, 4, 8, 16, 32, np.inf], labels=['-32+', '-32 to -16', '-16 to -8', '-8 to -4', '-4 to -2', '-2 to 0', '0 to 2', '2 to 4', '4 to 8', '8 to 16', '16 to 32', '32+'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab11fa7-e4ee-4ac3-a756-f2eaeb1a6bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df, x='profile_distance_bucket', y='treatment_delta')\n",
    "plt.title('Treatment Delta by Profile Distance Bucket (unt_net)')\n",
    "plt.xlabel('Profile Distance Bucket')\n",
    "plt.ylabel('Treatment Delta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fba09ff-30e7-4d06-bf39-eda0ee477341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['control_ROS'] = df['log_sales_pre']\n",
    "df['control_loadfactor'] = np.log(df['Loadfactor'] + 0.01)\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8501b556-bb8a-4a59-b58c-5f7a9ce087f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop('log_sales_pre', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bcfe98a-5208-4b1b-b129-8ac2b757daf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_first_stage = df[['Z_instrument', 'control_ROS', 'control_loadfactor', 'dtg']]\n",
    "y_first_stage = df['treatment_delta']\n",
    "\n",
    "X_first_stage = sm.add_constant(X_first_stage)\n",
    "\n",
    "model_first_stage = sm.OLS(y_first_stage, X_first_stage).fit()\n",
    "\n",
    "print(model_first_stage.summary())\n",
    "\n",
    "f_test = model_first_stage.f_test(\"Z_instrument = 0\")\n",
    "f_stat = f_test.fvalue\n",
    "\n",
    "print(f\"Instrument F-Statistic: {f_stat}\")\n",
    "\n",
    "if f_stat > 10:\n",
    "    print(\"PASS: Instrument is Strong. (F > 10)\")\n",
    "else:\n",
    "    print(\"FAIL: Instrument is Weak. (F < 10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e00cbcd-6a3d-4c24-a539-b541358138fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cyclic_cols = ['flight_dow', 'charge_dow', 'flight_dom', 'charge_dom', 'flight_mth', 'charge_mth']\n",
    "\n",
    "def encode_cyclic_features(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        max_val = df[col].max()\n",
    "        df[col + '_sin'] = (np.sin(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df[col + '_cos'] = (np.cos(2 * np.pi * df[col] / max_val)).astype('float16')\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = encode_cyclic_features(df, cyclic_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af8c00a-9cba-4515-bba0-4a719809935c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['day_number'] = (df['charge_dt'] - pd.to_datetime('2024-04-01')).dt.days.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f74bf91-0d1e-4524-a3a8-4d5efdf2d0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop('profile_distance_bucket', axis=1)\n",
    "optimize_df(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec4d9bd-0bcb-4b89-82d7-0c6c0ae6ae54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['charge_dt','flight_dt'])\n",
    "\n",
    "df_train = df[df['charge_dt'] < '2025-07-01']\n",
    "df_test = df[df['charge_dt'] >= '2025-07-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda69375-6f70-4163-b52d-eb8f987d2df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e91417-2a1d-40a4-bec2-c0b561c124b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f6e240-d1eb-4192-8d02-c44fb9a290a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85a67ac-9809-4c2e-abbb-b4527dd1c45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febc6ae3-dd96-4d4a-8012-b53d741f5482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "set_config(transform_output='pandas')\n",
    "\n",
    "cols_to_scale = ['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','sale_length','lag_sales_7','lag_sales_14','lag_sales_21','lag_sales_28','SF7C7','SF14C14','SF21C21','SF28C28','Loadfactor','control_loadfactor','control_ROS','sale_period_progress','new_cluster_label','lid','Z_instrument','treatment_delta','cumulative_sales','seats_remaining','day_number']\n",
    "\n",
    "preprocessor = ColumnTransformer([('scaler', StandardScaler(), cols_to_scale)], remainder='passthrough', verbose_feature_names_out=False )\n",
    "\n",
    "pipeline = Pipeline([('preprocessor', preprocessor)])\n",
    "\n",
    "df_train_scaled = pipeline.fit_transform(df_train)\n",
    "df_test_scaled = pipeline.transform(df_test)\n",
    "#df_train_scaled['dtg_sq_scaled'] = df_train_scaled['dtg'] ** 2\n",
    "#df_test_scaled['dtg_sq_scaled'] = df_test_scaled['dtg'] ** 2\n",
    "df_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c37534-3f68-4d08-ab95-d50dee945101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Select Nuissance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180f942c-62c1-4397-9b1e-7decfb962811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mlflow.set_experiment('/Workspace/Users/barney.hodge@easyjet.com/price-dependent-demand-forecast/Apprenticeship Project (elasticity estimate)')\n",
    "\n",
    "def get_residuals_cv(model_instance, target, controls, groups, n_splits=3):\n",
    "    cv = GroupKFold(n_splits=n_splits)\n",
    "    start_time = time.time()\n",
    "    predicted_baseline = cross_val_predict(model_instance, controls, target, cv=cv, groups=groups)\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    residuals = target - predicted_baseline\n",
    "    return residuals, predicted_baseline, duration\n",
    "\n",
    "model_candidates = {\n",
    "    'Ridge_Regression0.1': Ridge(alpha=0.01),\n",
    "    'Ridge_Regression10.0': Ridge(alpha=10),\n",
    "    'Lasso_Regression0.01': Lasso(alpha=0.01),\n",
    "    'XGBoost1': XGBRegressor(n_estimators=750, max_depth=5, learning_rate=0.01, verbosity=0, n_jobs=4),\n",
    "    'XGBoost2': XGBRegressor(n_estimators=750, max_depth=5, learning_rate=0.1, verbosity=0, n_jobs=4),\n",
    "    'XGBoost3': XGBRegressor(n_estimators=750, max_depth=5, learning_rate=1, verbosity=0, n_jobs=4),\n",
    "    'XGBoost4': XGBRegressor(n_estimators=750, max_depth=3, learning_rate=0.1, verbosity=0, n_jobs=4),\n",
    "    'XGBoost5': XGBRegressor(n_estimators=750, max_depth=7, learning_rate=0.1, verbosity=0, n_jobs=4),\n",
    "    'XGBoost6': XGBRegressor(n_estimators=750, max_depth=9, learning_rate=0.1, verbosity=0, n_jobs=4),\n",
    "    'XGBoost7': XGBRegressor(n_estimators=1000, max_depth=5, learning_rate=0.1, verbosity=0, n_jobs=4),\n",
    "    'XGBoost8': XGBRegressor(n_estimators=1500, max_depth=5, learning_rate=0.1, verbosity=0, n_jobs=4),\n",
    "    'XGBoost9': XGBRegressor(n_estimators=2000, max_depth=5, learning_rate=0.1, verbosity=0, n_jobs=4)\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f3735f-e34f-45e7-a002-fcd1868b2f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Y = df_train_scaled['outcome_delta'].values\n",
    "T = df_train_scaled['treatment_delta'].values\n",
    "Z = df_train_scaled['Z_instrument'].values\n",
    "\n",
    "W_names = ['flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','dtg','lag_sales_7','lag_sales_14','lag_sales_21','lag_sales_28', 'SF7C7','SF14C14','SF21C21','SF28C28','Loadfactor','control_loadfactor','cumulative_sales','control_ROS','sale_length','sale_period_progress','new_cluster_label','lid','is_charge_date_holiday','is_flight_date_holiday','seats_remaining']\n",
    "W = df_train_scaled[W_names].values\n",
    "X = df_train_scaled[['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','charge_dow_sin','charge_dow_cos']].values\n",
    "X_names = ['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','charge_dow_sin','charge_dow_cos']\n",
    "\n",
    "targets_dict = {'Y_outcome': Y}#, 'T_treatment': T, 'Z_instrument': Z}\n",
    "groups = df_train_scaled['flightkey'].values\n",
    "\n",
    "def extract_importance(model, feature_names):\n",
    "\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names})\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df['Importance'] = model.feature_importances_\n",
    "        importance_df['Type'] = 'Tree Gain/Split'\n",
    "        \n",
    "    elif hasattr(model, 'coef_'):\n",
    "        coefs = model.coef_\n",
    "        if coefs.ndim > 1: \n",
    "            coefs = coefs[0]\n",
    "        importance_df['Importance'] = coefs\n",
    "        importance_df['Type'] = 'Linear Coefficient'\n",
    "        \n",
    "    else:\n",
    "        importance_df['Importance'] = 0\n",
    "        importance_df['Type'] = 'Unknown'\n",
    "\n",
    "    importance_df['Abs_Importance'] = importance_df['Importance'].abs()\n",
    "    return importance_df.sort_values(by='Abs_Importance', ascending=False)\n",
    "\n",
    "# START EXPERIMENT LOOP\n",
    "for model_name, model_inst in model_candidates.items():\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "\n",
    "        print(f'Logging run for: {model_name}...')\n",
    "        mlflow.log_param('model_type', model_name)\n",
    "\n",
    "        if hasattr(model_inst, 'get_params'):\n",
    "            mlflow.log_params(model_inst.get_params())\n",
    "\n",
    "        total_duration = 0        \n",
    "        results_storage = {}\n",
    "\n",
    "        for target_name, target_data in targets_dict.items():\n",
    "\n",
    "            res, preds, duration = get_residuals_cv(model_inst, target_data, W, groups)\n",
    "            total_duration += duration\n",
    "            \n",
    "            rmse = np.sqrt(mean_squared_error(target_data, preds))\n",
    "            mae = mean_absolute_error(target_data, preds)\n",
    "            r2 = 1 - (np.sum(res**2) / np.sum((target_data - target_data.mean())**2))\n",
    "            \n",
    "            mlflow.log_metric(f'{target_name}_rmse', rmse)\n",
    "            mlflow.log_metric(f'{target_name}_mae', mae)\n",
    "            mlflow.log_metric(f'{target_name}_r2', r2)\n",
    "            \n",
    "            results_storage[target_name] = res\n",
    "\n",
    "            from sklearn.base import clone\n",
    "            explain_model = clone(model_inst) \n",
    "            explain_model.fit(W, target_data)\n",
    "            \n",
    "            # Extract\n",
    "            imp_df = extract_importance(explain_model, W_names)\n",
    "            \n",
    "            # Save CSV artifact (Raw numbers)\n",
    "            csv_name = f\"importance_{target_name}.csv\"\n",
    "            imp_df.to_csv(csv_name, index=False)\n",
    "            mlflow.log_artifact(csv_name)\n",
    "            \n",
    "            # Generate Plot (Top 20 features)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(\n",
    "                data=imp_df.head(20), \n",
    "                x='Importance', \n",
    "                y='Feature', \n",
    "                palette='viridis'\n",
    "            )\n",
    "            plt.title(f\"Top 20 Drivers for {target_name} \\n({model_name})\")\n",
    "            plt.axvline(0, color='k', linewidth=0.8) # Zero line for coefficients\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save Plot artifact\n",
    "            plot_name = f\"importance_plot_{target_name}.png\"\n",
    "            plt.savefig(plot_name)\n",
    "            mlflow.log_artifact(plot_name)\n",
    "            plt.close()\n",
    "            \n",
    "\n",
    "        mlflow.log_metric('total_cv_time_seconds', total_duration)\n",
    "        \n",
    "        # --- ARTIFACTS: RESIDUAL DISTRIBUTION ---\n",
    "        # A good residual should look like Gaussian noise centered at 0.\n",
    "        # If it's skewed, the model missed something.\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        for i, (key, val) in enumerate(results_storage.items()):\n",
    "            sns.histplot(val, kde=True, ax=axes[i], bins=50)\n",
    "            axes[i].set_title(f'{key} Residuhials')\n",
    "            axes[i].set_xlabel('Residual Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_name = 'residual_distributions.png'\n",
    "        plt.savefig(plot_name)\n",
    "        mlflow.log_artifact(plot_name)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f'Run {model_name} complete. Total CV Time: {total_duration:.2f}s')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34178457-1b7d-4ce4-b723-96e0d6fd1f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold, GroupKFold\n",
    "\n",
    "Y = df_train_scaled['outcome_delta'].values\n",
    "T = df_train_scaled['treatment_delta'].values\n",
    "Z = df_train_scaled['Z_instrument'].values\n",
    "\n",
    "W = df_train_scaled[['flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','dtg','lag_sales_7','lag_sales_14','lag_sales_21','lag_sales_28', 'SF7C7','SF14C14','SF21C21','SF28C28','Loadfactor','control_loadfactor','cumulative_sales','control_ROS','sale_length','sale_period_progress','new_cluster_label','lid','is_charge_date_holiday','is_flight_date_holiday','seats_remaining','day_number']].values\n",
    "\n",
    "X = df_train_scaled[['dtg','total_linear_optionality_score','combined_bp_DoW_DTG']].values\n",
    "X_names = ['dtg','total_linear_optionality_score','combined_bp_DoW_DTG']\n",
    "\n",
    "print(f\"Running Manual DML on {len(Y)} rows...\")\n",
    "print(\"Step 1: Cleaning Data (Orthogonalization)...\")\n",
    "\n",
    "groups = df_train_scaled['flightkey'].values\n",
    "\n",
    "def get_residuals(target, controls, groups):\n",
    "\n",
    "    xgbm = XGBRegressor(n_estimators=750, max_depth=7,learning_rate=0.1,verbose=0)\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    predicted_baseline = cross_val_predict(xgbm, controls, target, cv=cv, groups=groups, n_jobs=1)\n",
    "    return target - predicted_baseline\n",
    "\n",
    "y_res = get_residuals(Y, W, groups)\n",
    "t_res = get_residuals(T, W, groups)\n",
    "z_res = get_residuals(Z, W, groups)\n",
    "\n",
    "print(\"Step 2: Creating Interactions...\")\n",
    "\n",
    "T_interaction = t_res.reshape(-1, 1) * X\n",
    "T_final = np.column_stack([t_res, T_interaction])\n",
    "\n",
    "Z_interaction = z_res.reshape(-1, 1) * X\n",
    "Z_final = np.column_stack([z_res, Z_interaction])\n",
    "\n",
    "print(\"Step 3: Running Final IV Regression...\")\n",
    "\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "interaction_names = [f\"Price_x_{col}\" for col in X_names]\n",
    "exog_names = ['Price'] + interaction_names\n",
    "\n",
    "df_Y_res = pd.DataFrame(y_res, columns=['Sales_Res'])\n",
    "df_T_final = pd.DataFrame(T_final, columns=exog_names)\n",
    "df_Z_final = pd.DataFrame(Z_final, columns=[f\"Instr_{i}\" for i in range(Z_final.shape[1])])\n",
    "\n",
    "model = IV2SLS(dependent=df_Y_res, exog=None, endog=df_T_final, instruments=df_Z_final)\n",
    "\n",
    "results = model.fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c153b0-0e37-4d3f-8612-f20d97cc6448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "BETA_INTERCEPT = -0.2413 # price is inversely proportional to sales - elasticity model (expected)\n",
    "BETA_DTG = -0.0398 # when dtg increases, elasticty increases (expected)\n",
    "BETA_OPT = -0.0336 # when optionality increases, elasticity increases (expected)\n",
    "BETA_BP = 0.0486 # when business penetration increases, elasticity decreases (expected)\n",
    "\n",
    "def predict_elasticity(row):\n",
    "    e = BETA_INTERCEPT\n",
    "    e += BETA_DTG * row['dtg']                 \n",
    "    e += BETA_BP * row['combined_bp_DoW_DTG']     \n",
    "    e += BETA_OPT * row['total_linear_optionality_score']\n",
    "    return e\n",
    "\n",
    "# Apply to Test Set (Make sure it's scaled!)\n",
    "df_test_scaled['predicted_elasticity'] = df_test_scaled.apply(predict_elasticity, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. CREATE BUCKETS (QUINTILES)\n",
    "# ---------------------------------------------------------\n",
    "# Sort by predicted elasticity and split into 5 groups\n",
    "df_test_scaled['elasticity_bin'] = pd.qcut(df_test_scaled['predicted_elasticity'], 5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Buckets created. Calculating ACTUAL elasticity per bucket...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. VERIFY EACH BUCKET (THE TRUTH TEST)\n",
    "# ---------------------------------------------------------\n",
    "# We run a mini-IV regression for each bin to see the REAL slope.\n",
    "\n",
    "bucket_results = []\n",
    "bucket_labels = []\n",
    "\n",
    "for bin_num in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    # Filter data for this bucket\n",
    "    subset = df_test_scaled[df_test_scaled['elasticity_bin'] == bin_num]\n",
    "    \n",
    "    # Run simple IV: Sales ~ Price (Instrumented by Z)\n",
    "    # We use the same 'get_residuals' logic or just raw IV if sample is large enough\n",
    "    # Here we use raw IV for simplicity of the test check\n",
    "    iv_mod = IV2SLS(\n",
    "        dependent=subset['outcome_delta'],\n",
    "        exog=None,\n",
    "        endog=subset['treatment_delta'],\n",
    "        instruments=subset['Z_instrument']\n",
    "    ).fit()\n",
    "    \n",
    "    # The coefficient of 'treatment_delta' is the ACTUAL elasticity of this group\n",
    "    real_elasticity = iv_mod.params['treatment_delta']\n",
    "    bucket_results.append(real_elasticity)\n",
    "    bucket_labels.append(f\"Bin {bin_num}\")\n",
    "    \n",
    "    print(f\"Bin {bin_num} (Model pred: {subset['predicted_elasticity'].mean():.2f}) -> Actual Slope: {real_elasticity:.2f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VISUALIZE THE SUCCESS\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bucket_labels, bucket_results, color='skyblue', edgecolor='black')\n",
    "plt.ylabel('Actual Observed Elasticity (IV Slope)')\n",
    "plt.xlabel('Model Predicted Sensitivity (Bin 1 = Most Elastic)')\n",
    "plt.title('Model Validation: Did we correctly sort the flights?')\n",
    "plt.axhline(0, color='black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7cdcb8-355f-4dc0-ab64-e69982675f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#XGB\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold, GroupKFold\n",
    "\n",
    "Y = df_train_scaled['outcome_delta'].values\n",
    "T = df_train_scaled['treatment_delta'].values\n",
    "Z = df_train_scaled['Z_instrument'].values\n",
    "\n",
    "W = df_train_scaled[['routetype','region','flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','dtg','lag_sales_7','lag_sales_14','lag_sales_21','lag_sales_28', 'SF7C7','SF14C14','SF21C21','SF28C28','Loadfactor','control_loadfactor','cumulative_sales','control_ROS','sale_length','sale_period_progress','new_cluster_label','lid','is_charge_date_holiday','is_flight_date_holiday','seats_remaining','day_number']].values\n",
    "\n",
    "X = df_train_scaled[['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','charge_dow_sin','charge_dow_cos']].values\n",
    "X_names = ['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','charge_dow_sin','charge_dow_cos']\n",
    "\n",
    "print(f\"Running Manual DML on {len(Y)} rows...\")\n",
    "print(\"Step 1: Cleaning Data (Orthogonalization)...\")\n",
    "\n",
    "groups = df_train_scaled['flightkey'].values\n",
    "\n",
    "def get_residuals(target, controls, groups):\n",
    "\n",
    "    xgbm = XGBRegressor(n_estimators=750, max_depth=7,learning_rate=0.1,verbose=0)\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    predicted_baseline = cross_val_predict(xgbm, controls, target, cv=cv, groups=groups, n_jobs=1)\n",
    "    return target - predicted_baseline\n",
    "\n",
    "y_res = get_residuals(Y, W, groups)\n",
    "t_res = get_residuals(T, W, groups)\n",
    "z_res = get_residuals(Z, W, groups)\n",
    "\n",
    "print(\"Step 2: Creating Interactions...\")\n",
    "\n",
    "T_interaction = t_res.reshape(-1, 1) * X\n",
    "T_final = np.column_stack([t_res, T_interaction])\n",
    "\n",
    "Z_interaction = z_res.reshape(-1, 1) * X\n",
    "Z_final = np.column_stack([z_res, Z_interaction])\n",
    "\n",
    "print(\"Step 3: Running Final IV Regression...\")\n",
    "\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "interaction_names = [f\"Price_x_{col}\" for col in X_names]\n",
    "exog_names = ['Price'] + interaction_names\n",
    "\n",
    "df_Y_res = pd.DataFrame(y_res, columns=['Sales_Res'])\n",
    "df_T_final = pd.DataFrame(T_final, columns=exog_names)\n",
    "df_Z_final = pd.DataFrame(Z_final, columns=[f\"Instr_{i}\" for i in range(Z_final.shape[1])])\n",
    "\n",
    "model = IV2SLS(dependent=df_Y_res, exog=None, endog=df_T_final, instruments=df_Z_final)\n",
    "\n",
    "results = model.fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a274167-a1ca-42d7-89e7-5662bc62b4d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "BETA_INTERCEPT = -0.3196 # price is inversely proportional to sales - elasticity model (expected)\n",
    "BETA_DTG = -0.0462 # when dtg increases, elasticty increases (expected)\n",
    "BETA_OPT = -0.0128 # when optionality increases, elasticity increases (expected)\n",
    "BETA_BP = 0.0285 # when business penetration increases, elasticity decreases (expected)\n",
    "BETA_CDOW_SIN = 0.0266\n",
    "BETA_CDOW_COS = -0.0425\n",
    "\n",
    "\n",
    "\n",
    "def predict_elasticity(row):\n",
    "    e = BETA_INTERCEPT\n",
    "    e += BETA_DTG * row['dtg']                 \n",
    "    e += BETA_BP * row['combined_bp_DoW_DTG']     \n",
    "    e += BETA_OPT * row['total_linear_optionality_score']\n",
    "    e += BETA_CDOW_SIN * row['charge_dow_sin']\n",
    "    e += BETA_CDOW_COS * row['charge_dow_cos'] \n",
    "    return e\n",
    "\n",
    "# Apply to Test Set (Make sure it's scaled!)\n",
    "df_test_scaled['predicted_elasticity'] = df_test_scaled.apply(predict_elasticity, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. CREATE BUCKETS (QUINTILES)\n",
    "# ---------------------------------------------------------\n",
    "# Sort by predicted elasticity and split into 5 groups\n",
    "df_test_scaled['elasticity_bin'] = pd.qcut(df_test_scaled['predicted_elasticity'], 5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Buckets created. Calculating ACTUAL elasticity per bucket...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. VERIFY EACH BUCKET (THE TRUTH TEST)\n",
    "# ---------------------------------------------------------\n",
    "# We run a mini-IV regression for each bin to see the REAL slope.\n",
    "\n",
    "bucket_results = []\n",
    "bucket_labels = []\n",
    "\n",
    "for bin_num in [1, 2, 3, 4, 5]:\n",
    "    # Filter data for this bucket\n",
    "    subset = df_test_scaled[df_test_scaled['elasticity_bin'] == bin_num]\n",
    "    \n",
    "    # Run simple IV: Sales ~ Price (Instrumented by Z)\n",
    "    # We use the same 'get_residuals' logic or just raw IV if sample is large enough\n",
    "    # Here we use raw IV for simplicity of the test check\n",
    "    iv_mod = IV2SLS(\n",
    "        dependent=subset['outcome_delta'],\n",
    "        exog=None,\n",
    "        endog=subset['treatment_delta'],\n",
    "        instruments=subset['Z_instrument']\n",
    "    ).fit()\n",
    "    \n",
    "    # The coefficient of 'treatment_delta' is the ACTUAL elasticity of this group\n",
    "    real_elasticity = iv_mod.params['treatment_delta']\n",
    "    bucket_results.append(real_elasticity)\n",
    "    bucket_labels.append(f\"Bin {bin_num}\")\n",
    "    \n",
    "    print(f\"Bin {bin_num} (Model pred: {subset['predicted_elasticity'].mean():.2f}) -> Actual Slope: {real_elasticity:.2f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VISUALIZE THE SUCCESS\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bucket_labels, bucket_results, color='skyblue', edgecolor='black')\n",
    "plt.ylabel('Actual Observed Elasticity (IV Slope)')\n",
    "plt.xlabel('Model Predicted Sensitivity (Bin 1 = Most Elastic)')\n",
    "plt.title('Model Validation: Did we correctly sort the flights?')\n",
    "plt.axhline(0, color='black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b3e586-a4a2-4b69-b999-f3363b0919c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#XGB\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold, GroupKFold\n",
    "\n",
    "Y = df_train_scaled['outcome_delta'].values\n",
    "T = df_train_scaled['treatment_delta'].values\n",
    "Z = df_train_scaled['Z_instrument'].values\n",
    "\n",
    "W = df_train_scaled[['region','routetype','flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','dtg','lag_sales_7','lag_sales_14','lag_sales_21','lag_sales_28', 'SF7C7','SF14C14','SF21C21','SF28C28','Loadfactor','control_loadfactor','cumulative_sales','control_ROS','sale_length','sale_period_progress','new_cluster_label','lid','is_charge_date_holiday','is_flight_date_holiday','seats_remaining']].values\n",
    "\n",
    "X = df_train_scaled[['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','charge_dow_sin','charge_dow_cos','charge_dom_sin','charge_dom_cos']].values\n",
    "X_names = ['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','charge_dow_sin','charge_dow_cos','charge_dom_sin','charge_dom_cos']\n",
    "\n",
    "print(f\"Running Manual DML on {len(Y)} rows...\")\n",
    "print(\"Step 1: Cleaning Data (Orthogonalization)...\")\n",
    "\n",
    "groups = df_train_scaled['flightkey'].values\n",
    "\n",
    "def get_residuals(target, controls, groups):\n",
    "\n",
    "    xgbm = XGBRegressor(n_estimators=1000, max_depth=5,learning_rate=0.1,verbose=-1)\n",
    "    cv = GroupKFold(n_splits=3)\n",
    "    predicted_baseline = cross_val_predict(xgbm, controls, target, cv=cv, groups=groups, n_jobs=1)\n",
    "    return target - predicted_baseline\n",
    "\n",
    "y_res = get_residuals(Y, W, groups)\n",
    "t_res = get_residuals(T, W, groups)\n",
    "z_res = get_residuals(Z, W, groups)\n",
    "\n",
    "print(\"Step 2: Creating Interactions...\")\n",
    "\n",
    "T_interaction = t_res.reshape(-1, 1) * X\n",
    "T_final = np.column_stack([t_res, T_interaction])\n",
    "\n",
    "Z_interaction = z_res.reshape(-1, 1) * X\n",
    "Z_final = np.column_stack([z_res, Z_interaction])\n",
    "\n",
    "print(\"Step 3: Running Final IV Regression...\")\n",
    "\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "interaction_names = [f\"Price_x_{col}\" for col in X_names]\n",
    "exog_names = ['Price'] + interaction_names\n",
    "\n",
    "df_Y_res = pd.DataFrame(y_res, columns=['Sales_Res'])\n",
    "df_T_final = pd.DataFrame(T_final, columns=exog_names)\n",
    "df_Z_final = pd.DataFrame(Z_final, columns=[f\"Instr_{i}\" for i in range(Z_final.shape[1])])\n",
    "\n",
    "model = IV2SLS(dependent=df_Y_res, exog=None, endog=df_T_final, instruments=df_Z_final)\n",
    "\n",
    "results = model.fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0537c943-5de2-43a5-b1a2-b26391f9bd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BETA_INTERCEPT = -0.3216 # price is inversely proportional to sales - elasticity model (expected)\n",
    "BETA_DTG = -0.0460 # when dtg increases, elasticty increases (expected)\n",
    "BETA_OPT = -0.0134 # when optionality increases, elasticity increases (expected)\n",
    "BETA_BP = 0.0280 # when business penetration increases, elasticity decreases (expected)\n",
    "BETA_CDOW_SIN = 0.0268\n",
    "BETA_CDOW_COS = -0.0425\n",
    "BETA_CDOM_SIN = 0.0251\n",
    "BETA_CDOM_COS = -0.0145\n",
    "\n",
    "\n",
    "\n",
    "def predict_elasticity(row):\n",
    "    e = BETA_INTERCEPT\n",
    "    e += BETA_DTG * row['dtg']                 \n",
    "    e += BETA_BP * row['combined_bp_DoW_DTG']     \n",
    "    e += BETA_OPT * row['total_linear_optionality_score']\n",
    "    e += BETA_CDOW_SIN * row['charge_dow_sin']\n",
    "    e += BETA_CDOW_COS * row['charge_dow_cos']\n",
    "    e += BETA_CDOM_SIN * row['charge_dom_sin']\n",
    "    e += BETA_CDOM_COS * row['charge_dom_cos']\n",
    "    return e\n",
    "\n",
    "# Apply to Test Set (Make sure it's scaled!)\n",
    "df_test_scaled['predicted_elasticity'] = df_test_scaled.apply(predict_elasticity, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. CREATE BUCKETS (QUINTILES)\n",
    "# ---------------------------------------------------------\n",
    "# Sort by predicted elasticity and split into 5 groups\n",
    "df_test_scaled['elasticity_bin'] = pd.qcut(df_test_scaled['predicted_elasticity'], 5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Buckets created. Calculating ACTUAL elasticity per bucket...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. VERIFY EACH BUCKET (THE TRUTH TEST)\n",
    "# ---------------------------------------------------------\n",
    "# We run a mini-IV regression for each bin to see the REAL slope.\n",
    "\n",
    "bucket_results = []\n",
    "bucket_labels = []\n",
    "\n",
    "for bin_num in [1, 2, 3, 4, 5]:\n",
    "    # Filter data for this bucket\n",
    "    subset = df_test_scaled[df_test_scaled['elasticity_bin'] == bin_num]\n",
    "    \n",
    "    # Run simple IV: Sales ~ Price (Instrumented by Z)\n",
    "    # We use the same 'get_residuals' logic or just raw IV if sample is large enough\n",
    "    # Here we use raw IV for simplicity of the test check\n",
    "    iv_mod = IV2SLS(\n",
    "        dependent=subset['outcome_delta'],\n",
    "        exog=None,\n",
    "        endog=subset['treatment_delta'],\n",
    "        instruments=subset['Z_instrument']\n",
    "    ).fit()\n",
    "    \n",
    "    # The coefficient of 'treatment_delta' is the ACTUAL elasticity of this group\n",
    "    real_elasticity = iv_mod.params['treatment_delta']\n",
    "    bucket_results.append(real_elasticity)\n",
    "    bucket_labels.append(f\"Bin {bin_num}\")\n",
    "    \n",
    "    print(f\"Bin {bin_num} (Model pred: {subset['predicted_elasticity'].mean():.2f}) -> Actual Slope: {real_elasticity:.2f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VISUALIZE THE SUCCESS\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bucket_labels, bucket_results, color='skyblue', edgecolor='black')\n",
    "plt.ylabel('Actual Observed Elasticity (IV Slope)')\n",
    "plt.xlabel('Model Predicted Sensitivity (Bin 1 = Most Elastic)')\n",
    "plt.title('Model Validation: Did we correctly sort the flights?')\n",
    "plt.axhline(0, color='black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb48f7ad-bc67-427b-94b4-2897f5f70ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold, GroupKFold\n",
    "\n",
    "Y = df_train_scaled['outcome_delta'].values\n",
    "T = df_train_scaled['treatment_delta'].values\n",
    "Z = df_train_scaled['Z_instrument'].values\n",
    "\n",
    "W = df_train_scaled[['flight_dow_sin','flight_dow_cos','charge_dow_sin','charge_dow_cos','flight_dom_sin','flight_dom_cos','charge_dom_sin','charge_dom_cos','flight_mth_sin','flight_mth_cos','charge_mth_sin','charge_mth_cos','dtg','lag_sales_7','lag_sales_14','lag_sales_21','lag_sales_28', 'SF7C7','SF14C14','SF21C21','SF28C28','Loadfactor','control_loadfactor','cumulative_sales','control_ROS','sale_length','sale_period_progress','new_cluster_label','lid','is_charge_date_holiday','is_flight_date_holiday','seats_remaining']].values\n",
    "\n",
    "X = df_train_scaled[['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','charge_dow_sin','charge_dow_cos']].values\n",
    "X_names = ['dtg','total_linear_optionality_score','combined_bp_DoW_DTG','charge_dow_sin','charge_dow_cos']\n",
    "\n",
    "print(f\"Running Manual DML on {len(Y)} rows...\")\n",
    "print(\"Step 1: Cleaning Data (Orthogonalization)...\")\n",
    "\n",
    "groups = df_train_scaled['flightkey'].values\n",
    "\n",
    "def get_residuals(target, controls, groups):\n",
    "\n",
    "    xgbm = XGBRegressor(n_estimators=2000, max_depth=7,learning_rate=0.1,verbose=-1)\n",
    "    cv = GroupKFold(n_splits=5)\n",
    "    predicted_baseline = cross_val_predict(xgbm, controls, target, cv=cv, groups=groups, n_jobs=1)\n",
    "    return target - predicted_baseline\n",
    "\n",
    "y_res = get_residuals(Y, W, groups)\n",
    "t_res = get_residuals(T, W, groups)\n",
    "z_res = get_residuals(Z, W, groups)\n",
    "\n",
    "print(\"Step 2: Creating Interactions...\")\n",
    "\n",
    "T_interaction = t_res.reshape(-1, 1) * X\n",
    "T_final = np.column_stack([t_res, T_interaction])\n",
    "\n",
    "Z_interaction = z_res.reshape(-1, 1) * X\n",
    "Z_final = np.column_stack([z_res, Z_interaction])\n",
    "\n",
    "print(\"Step 3: Running Final IV Regression...\")\n",
    "\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "interaction_names = [f\"Price_x_{col}\" for col in X_names]\n",
    "exog_names = ['Price'] + interaction_names\n",
    "\n",
    "df_Y_res = pd.DataFrame(y_res, columns=['Sales_Res'])\n",
    "df_T_final = pd.DataFrame(T_final, columns=exog_names)\n",
    "df_Z_final = pd.DataFrame(Z_final, columns=[f\"Instr_{i}\" for i in range(Z_final.shape[1])])\n",
    "\n",
    "model = IV2SLS(dependent=df_Y_res, exog=None, endog=df_T_final, instruments=df_Z_final)\n",
    "\n",
    "results = model.fit()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15918040-a185-466c-af3e-ea2746d16974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BETA_INTERCEPT = -0.3601 # price is inversely proportional to sales - elasticity model (expected)\n",
    "BETA_DTG = -0.1126 # when dtg increases, elasticty increases (expected)\n",
    "BETA_OPT = -0.0297 # when optionality increases, elasticity increases (expected)\n",
    "BETA_BP = 0.0301 # when business penetration increases, elasticity decreases (expected)\n",
    "BETA_CDOW_SIN = 0.0346\n",
    "BETA_CDOW_COS = -0.0323\n",
    "\n",
    "\n",
    "def predict_elasticity(row):\n",
    "    e = BETA_INTERCEPT\n",
    "    e += BETA_DTG * row['dtg']                 \n",
    "    e += BETA_BP * row['combined_bp_DoW_DTG']     \n",
    "    e += BETA_OPT * row['total_linear_optionality_score']\n",
    "    e += BETA_CDOW_SIN * row['charge_dow_sin']\n",
    "    e += BETA_CDOW_COS * row['charge_dow_cos'] \n",
    "    return e\n",
    "\n",
    "# Apply to Test Set (Make sure it's scaled!)\n",
    "df_test_scaled['predicted_elasticity'] = df_test_scaled.apply(predict_elasticity, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. CREATE BUCKETS (QUINTILES)\n",
    "# ---------------------------------------------------------\n",
    "# Sort by predicted elasticity and split into 5 groups\n",
    "df_test_scaled['elasticity_bin'] = pd.qcut(df_test_scaled['predicted_elasticity'], 5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Buckets created. Calculating ACTUAL elasticity per bucket...\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. VERIFY EACH BUCKET (THE TRUTH TEST)\n",
    "# ---------------------------------------------------------\n",
    "# We run a mini-IV regression for each bin to see the REAL slope.\n",
    "\n",
    "bucket_results = []\n",
    "bucket_labels = []\n",
    "\n",
    "for bin_num in [1, 2, 3, 4, 5]:\n",
    "    # Filter data for this bucket\n",
    "    subset = df_test_scaled[df_test_scaled['elasticity_bin'] == bin_num]\n",
    "    \n",
    "    # Run simple IV: Sales ~ Price (Instrumented by Z)\n",
    "    # We use the same 'get_residuals' logic or just raw IV if sample is large enough\n",
    "    # Here we use raw IV for simplicity of the test check\n",
    "    iv_mod = IV2SLS(\n",
    "        dependent=subset['outcome_delta'],\n",
    "        exog=None,\n",
    "        endog=subset['treatment_delta'],\n",
    "        instruments=subset['Z_instrument']\n",
    "    ).fit()\n",
    "    \n",
    "    # The coefficient of 'treatment_delta' is the ACTUAL elasticity of this group\n",
    "    real_elasticity = iv_mod.params['treatment_delta']\n",
    "    bucket_results.append(real_elasticity)\n",
    "    bucket_labels.append(f\"Bin {bin_num}\")\n",
    "    \n",
    "    print(f\"Bin {bin_num} (Model pred: {subset['predicted_elasticity'].mean():.2f}) -> Actual Slope: {real_elasticity:.2f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VISUALIZE THE SUCCESS\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bucket_labels, bucket_results, color='skyblue', edgecolor='black')\n",
    "plt.ylabel('Actual Observed Elasticity (IV Slope)')\n",
    "plt.xlabel('Model Predicted Sensitivity (Bin 1 = Most Elastic)')\n",
    "plt.title('Model Validation: Did we correctly sort the flights?')\n",
    "plt.axhline(0, color='black', linewidth=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apprenticeship Project (elasticity estimate)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
